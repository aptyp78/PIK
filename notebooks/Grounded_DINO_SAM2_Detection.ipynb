{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aptyp78/PIKAi/blob/colab-latest/notebooks/Grounded_DINO_SAM2_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d596de28",
   "metadata": {
    "id": "d596de28"
   },
   "source": [
    "# GroundedDINO + SAM — Detection (Colab Pro+) — commit afaa196 — 2025-09-16T18:30:50Z\n",
    "Детектор регионов: монтируем GCS (через сервис‑аккаунт), ставим Torch+детекторы, рендерим страницы, запускаем детекцию и грузим регионы в `gs://pik-artifacts-dev/grounded_regions/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b042689",
   "metadata": {
    "id": "3b042689"
   },
   "outputs": [],
   "source": [
    "#@title No-op gate helper\n",
    "def require_start():\n",
    "  return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76900c57",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "76900c57",
    "outputId": "5d9102a0-cc4d-42c6-fcc8-ddcb4393b2ec"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[cell-logger] enabled -> /content/colab_runs/20250916-161108/cells.jsonl\n"
     ]
    }
   ],
   "source": [
    "#@title Cell Execution Logger\n",
    "import os, sys, json, time, uuid, warnings\n",
    "from pathlib import Path\n",
    "try:\n",
    "  LOG_DIR  # noqa: F821\n",
    "except NameError:\n",
    "  RUN_ID = time.strftime('%Y%m%d-%H%M%S')\n",
    "  LOCAL_LOG_ROOT = '/content/colab_runs'\n",
    "  LOG_DIR = Path(LOCAL_LOG_ROOT)/RUN_ID\n",
    "  LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from IPython import get_ipython\n",
    "ip = get_ipython()\n",
    "\n",
    "class _Tee:\n",
    "  def __init__(self, stream, buf_list):\n",
    "    self._s = stream; self._b = buf_list\n",
    "  def write(self, s):\n",
    "    try: self._s.write(s)\n",
    "    finally: self._b.append(s)\n",
    "  def flush(self):\n",
    "    try: self._s.flush()\n",
    "    except Exception: pass\n",
    "\n",
    "_celllog = {'i': None, 'start': None, 'buf_out':[], 'buf_err':[], 'warns':[], 'id': None}\n",
    "_orig_out, _orig_err = sys.stdout, sys.stderr\n",
    "_orig_showwarning = warnings.showwarning\n",
    "LOG_JSONL = str(LOG_DIR/'cells.jsonl')\n",
    "\n",
    "def _pre(cell_id):\n",
    "  _celllog['i'] = ip.execution_count + 1\n",
    "  _celllog['id'] = str(uuid.uuid4())\n",
    "  _celllog['start'] = time.time()\n",
    "  _celllog['buf_out'] = []\n",
    "  _celllog['buf_err'] = []\n",
    "  _celllog['warns'] = []\n",
    "  sys.stdout = _Tee(_orig_out, _celllog['buf_out'])\n",
    "  sys.stderr = _Tee(_orig_err, _celllog['buf_err'])\n",
    "  def _sw(message, category, filename, lineno, file=None, line=None):\n",
    "    _celllog['warns'].append({'message': str(message), 'category': getattr(category,'__name__', str(category)), 'filename': filename, 'lineno': lineno})\n",
    "    return _orig_showwarning(message, category, filename, lineno, file, line)\n",
    "  warnings.showwarning = _sw\n",
    "\n",
    "def _post(result):\n",
    "  # restore\n",
    "  sys.stdout = _orig_out\n",
    "  sys.stderr = _orig_err\n",
    "  warnings.showwarning = _orig_showwarning\n",
    "  end = time.time()\n",
    "  i = _celllog.get('i')\n",
    "  # Try to get cell source from history\n",
    "  src = None\n",
    "  try:\n",
    "    ih = ip.user_ns.get('_ih', [])\n",
    "    if i is not None and i < len(ih):\n",
    "      src = ih[i]\n",
    "  except Exception:\n",
    "    src = None\n",
    "  rec = {\n",
    "    'cell_id': _celllog.get('id'),\n",
    "    'execution_count': i,\n",
    "    'start_ts': _celllog.get('start'),\n",
    "    'end_ts': end,\n",
    "    'duration_s': (end - _celllog['start']) if _celllog.get('start') else None,\n",
    "    'success': bool(getattr(result, 'success', True)),\n",
    "    'out': ''.join(_celllog.get('buf_out') or []),\n",
    "    'err': ''.join(_celllog.get('buf_err') or []),\n",
    "    'warnings': _celllog.get('warns') or [],\n",
    "    'source': src,\n",
    "  }\n",
    "  try:\n",
    "    with open(LOG_JSONL, 'a', encoding='utf-8') as f:\n",
    "      f.write(json.dumps(rec, ensure_ascii=False) + '\\n')\n",
    "  except Exception as e:\n",
    "    print('[cell-logger] write failed:', e)\n",
    "\n",
    "ip.events.register('pre_run_cell', _pre)\n",
    "ip.events.register('post_run_cell', _post)\n",
    "print('[cell-logger] enabled ->', LOG_JSONL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d91d7fd2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d91d7fd2",
    "outputId": "aa96b0e5-88f0-4060-cd6c-8a42434903ce"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Notebook version: 2652b08\n",
      "Tue Sep 16 16:11:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n"
     ]
    }
   ],
   "source": [
    "#@title Runtime & GPU\n",
    "NOTEBOOK_VERSION = 'afaa196'\n",
    "print('Notebook version:', NOTEBOOK_VERSION)\n",
    "# Runtime & GPU\n",
    "!nvidia-smi || true\n",
    "import sys; print(sys.version)\n",
    "NOTEBOOK_UPDATED = '2025-09-16T18:30:50Z'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd390ea",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cfd390ea",
    "outputId": "e6686a29-c5a0-407b-dfcc-1bf23072d822"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[auth] Colab user credentials OK\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:4 https://cli.github.com/packages stable InRelease [3,917 B]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "Get:6 https://packages.cloud.google.com/apt gcsfuse-jammy InRelease [1,227 B]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Get:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
      "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:11 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:12 https://cli.github.com/packages stable/main amd64 Packages [346 B]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,581 kB]\n",
      "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,006 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,624 kB]\n",
      "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [5,627 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [88.8 kB]\n",
      "Get:19 https://packages.cloud.google.com/apt gcsfuse-jammy/main amd64 Packages [45.2 kB]\n",
      "Get:20 https://packages.cloud.google.com/apt gcsfuse-jammy/main all Packages [750 B]\n",
      "Get:21 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [43.2 kB]\n",
      "Get:22 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,441 kB]\n",
      "Get:23 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,799 kB]\n",
      "Get:24 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,311 kB]\n",
      "Get:25 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [80.3 kB]\n",
      "Get:26 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,274 kB]\n",
      "Get:27 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,278 kB]\n",
      "Fetched 35.6 MB in 3s (13.2 MB/s)\n",
      "Reading package lists...\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following NEW packages will be installed:\n",
      "  gcsfuse poppler-utils\n",
      "0 upgraded, 2 newly installed, 0 to remove and 50 not upgraded.\n",
      "Need to get 15.3 MB of archives.\n",
      "After this operation, 697 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.10 [186 kB]\n",
      "Get:2 https://packages.cloud.google.com/apt gcsfuse-jammy/main amd64 gcsfuse amd64 3.3.0 [15.1 MB]\n",
      "Fetched 15.3 MB in 0s (57.1 MB/s)\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
      "debconf: falling back to frontend: Readline\n",
      "debconf: unable to initialize frontend: Readline\n",
      "debconf: (This frontend requires a controlling tty.)\n",
      "debconf: falling back to frontend: Teletype\n",
      "dpkg-preconfigure: unable to re-open stdin: \n",
      "Selecting previously unselected package poppler-utils.\n",
      "(Reading database ... 126374 files and directories currently installed.)\n",
      "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.10_amd64.deb ...\n",
      "Unpacking poppler-utils (22.02.0-2ubuntu0.10) ...\n",
      "Selecting previously unselected package gcsfuse.\n",
      "Preparing to unpack .../gcsfuse_3.3.0_amd64.deb ...\n",
      "Unpacking gcsfuse (3.3.0) ...\n",
      "Setting up gcsfuse (3.3.0) ...\n",
      "Setting up poppler-utils (22.02.0-2ubuntu0.10) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "[auth] SA key written to /content/Secrets/sa.json (source: GCS_SA_JSON )\n",
      "[auth] SA prepared. Proceed to mount in the next cell.\n"
     ]
    }
   ],
   "source": [
    "#@title Auth + gcsfuse setup\n",
    "require_start()\n",
    "\n",
    "# Install packages and prepare gcsfuse repo; auto-mount with SA from /content/Secrets if present\n",
    "# Try Colab user auth (optional)\n",
    "try:\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "  print('[auth] Colab user credentials OK')\n",
    "except Exception as e:\n",
    "  print('[auth] Skipping Colab user auth:', e)\n",
    "!pip -q install google-cloud-storage gcsfs==2025.3.0 fsspec==2025.3.0\n",
    "!sudo install -m 0755 -d /usr/share/keyrings\n",
    "!curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg -o /tmp/cloud.google.gpg\n",
    "!sudo gpg --dearmor --yes --batch -o /usr/share/keyrings/cloud.google.gpg /tmp/cloud.google.gpg || sudo cp /tmp/cloud.google.gpg /usr/share/keyrings/cloud.google.gpg\n",
    "!echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt gcsfuse-jammy main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list >/dev/null\n",
    "!sudo apt-get -q update\n",
    "!sudo apt-get -q install -y gcsfuse poppler-utils\n",
    "!mkdir -p /content/src_gcs /content/artifacts /content/pages /content/Secrets\n",
    "import glob, os, subprocess, shlex\n",
    "\n",
    "# Read SA key from Colab Secrets (GCS_SA_JSON / GCS_SA_JSON2 / secretName) and optional GOOGLE_API_KEY\n",
    "try:\n",
    "  from google.colab import userdata as _ud\n",
    "  _sa = None; _sa_name = ''\n",
    "  for _k in ('GCS_SA_JSON','GCS_SA_JSON2','secretName'):\n",
    "    try:\n",
    "      _v = _ud.get(_k)\n",
    "    except Exception:\n",
    "      _v = None\n",
    "    if _v:\n",
    "      _sa = _v; _sa_name = _k; break\n",
    "  try:\n",
    "    _gapi = _ud.get('GOOGLE_API_KEY')\n",
    "  except Exception:\n",
    "    _gapi = None\n",
    "except Exception:\n",
    "  _sa = None; _gapi = None; _sa_name = ''\n",
    "if _gapi:\n",
    "  os.environ['GOOGLE_API_KEY'] = _gapi\n",
    "  print('[auth] GOOGLE_API_KEY loaded from Colab Secrets')\n",
    "if _sa:\n",
    "  os.makedirs('/content/Secrets', exist_ok=True)\n",
    "  _key_path = '/content/Secrets/sa.json'\n",
    "  open(_key_path,'w',encoding='utf-8').write(_sa)\n",
    "  os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = _key_path\n",
    "  print('[auth] SA key written to', _key_path, '(source:', _sa_name, ')')\n",
    "\n",
    "# Skip auto-mount here; use the next cell 'Mount GCS buckets'\n",
    "print('[auth] SA prepared. Proceed to mount in the next cell.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3076737",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3076737",
    "outputId": "9ea71840-e5d4-45b7-be5f-7937344db432"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m[compat] jedi= not installed typing_extensions= 4.15.0 filelock= 3.19.1 numpy= 2.0.2 gcsfs= 2025.3.0 fsspec= 2025.3.0\n"
     ]
    }
   ],
   "source": [
    "#@title Compatibility Fixes (pip pins)\n",
    "require_start()\n",
    "# Do not upgrade ipython (Colab expects ipython==7.34.0)\n",
    "# Remove xformers if present (version coupling to torch; optional)\n",
    "!pip -q uninstall -y xformers || true\n",
    "\n",
    "# Pin compatible versions for known conflicts\n",
    "!pip -q install -U   \"typing_extensions>=4.14.0,<5\"   \"filelock>=3.15\"   \"numpy<2.1,>=1.24\"   gcsfs==2025.3.0 fsspec==2025.3.0\n",
    "\n",
    "from importlib import metadata as md\n",
    "def _ver(name, mod=None):\n",
    "    try:\n",
    "        return md.version(name)\n",
    "    except Exception:\n",
    "        try:\n",
    "            m = __import__(mod or name)\n",
    "            return getattr(m, \"__version__\", \"unknown\")\n",
    "        except Exception:\n",
    "            return \"not installed\"\n",
    "print(\n",
    "  \"[compat]\",\n",
    "  \"jedi=\", _ver(\"jedi\"),\n",
    "  \"typing_extensions=\", _ver(\"typing_extensions\",\"typing_extensions\"),\n",
    "  \"filelock=\", _ver(\"filelock\"),\n",
    "  \"numpy=\", _ver(\"numpy\"),\n",
    "  \"gcsfs=\", _ver(\"gcsfs\"),\n",
    "  \"fsspec=\", _ver(\"fsspec\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9813e96",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a9813e96",
    "outputId": "77ed4fc7-8c12-4e9c-ed22-9cf1555ad3bc"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using SA key: /content/Secrets/sa.json\n",
      "[mount] pik_source_bucket -> /content/src_gcs: True\n",
      "[mount] pik-artifacts-dev -> /content/artifacts: True\n",
      "mount src= True  mount artifacts= True\n"
     ]
    }
   ],
   "source": [
    "#@title Mount GCS buckets\n",
    "require_start()\n",
    "\n",
    "# Robust mount with verbose logs and fallback info\n",
    "import os, glob, subprocess, pathlib, textwrap\n",
    "pathlib.Path('/content/src_gcs').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('/content/artifacts').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('/content/gcsfuse_tmp').mkdir(parents=True, exist_ok=True)\n",
    "key = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS', '')\n",
    "if (not key) and 'KEY' in globals(): key = KEY\n",
    "if key and not os.path.isabs(key): key = os.path.join('/content', key)\n",
    "if not (key and os.path.exists(key)):\n",
    "  matches = glob.glob('/content/Secrets/*.json')\n",
    "  if matches:\n",
    "    key = matches[0]; os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = key\n",
    "  else:\n",
    "    raise SystemExit('Service account key not found — upload to /content/Secrets/*.json')\n",
    "print('Using SA key:', key)\n",
    "def mount(bucket, mnt):\n",
    "  log=f'/content/gcsfuse_{bucket}.log'.replace('/', '_')\n",
    "  cmd=['gcsfuse','--implicit-dirs','--key-file', key,'--temp-dir','/content/gcsfuse_tmp','--log-file',log, bucket, mnt]\n",
    "  res=subprocess.run(cmd, capture_output=True, text=True)\n",
    "  ok=(res.returncode==0)\n",
    "  print(f'[mount] {bucket} -> {mnt}:', ok)\n",
    "  if not ok:\n",
    "    print('[mount] stdout:\\n' + res.stdout)\n",
    "    print('[mount] stderr:\\n' + res.stderr)\n",
    "    try:\n",
    "      tail=subprocess.run(['bash','-lc', f'tail -n 80 {log}'], capture_output=True, text=True)\n",
    "      if tail.stdout: print('[mount] log tail:\\n' + tail.stdout)\n",
    "    except Exception: pass\n",
    "  return ok\n",
    "subprocess.run(['fusermount','-u','/content/src_gcs'], check=False)\n",
    "subprocess.run(['fusermount','-u','/content/artifacts'], check=False)\n",
    "# Quick bucket existence check\n",
    "for b in ('pik_source_bucket','pik-artifacts-dev'):\n",
    "  subprocess.run(['bash','-lc', f'gsutil ls -b gs://{b} || true'], check=False)\n",
    "ok1 = mount('pik_source_bucket','/content/src_gcs')\n",
    "ok2 = mount('pik-artifacts-dev','/content/artifacts')\n",
    "print('mount src=', ok1, ' mount artifacts=', ok2)\n",
    "if not (ok1 and ok2):\n",
    "  print(textwrap.dedent('''\n",
    "    [hint] If mount keeps failing:\n",
    "      - Check SA has Storage Object Admin on both buckets\n",
    "      - Try fallback: copy files with gsutil (already used elsewhere in the notebook)\n",
    "      - Ensure bucket names are correct and exist (see checks above)\n",
    "    '''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6309994d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6309994d",
    "outputId": "6acc06d3-b636-4b84-814f-71f439465cdd"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[torch] installing for cu121\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "typeguard 4.4.4 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
      "pytensor 2.31.7 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.2 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytensor 2.31.7 requires filelock>=3.15, but you have filelock 3.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "[GroundingDINO] installing via pip…\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "✅ GroundingDINO pip import OK\n",
      "[versions] torch: 2.5.1+cu121, numpy: 2.0.2\n",
      "✅ GroundingDINO.Model available\n",
      "✅ SAM v1 import OK\n",
      "✅ SAM2 import OK\n",
      "CUDA available: True\n",
      "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#@title Install Torch + SAM/SAM2 + GroundedDINO (CUDA-aware)\n",
    "require_start()\n",
    "\n",
    "# 1) Update base tooling\n",
    "!pip -q install --upgrade pip setuptools wheel\n",
    "!pip -q install -U jedi>=0.16 typing_extensions>=4.14.0 filelock>=3.15\n",
    "\n",
    "# 2) Torch for detected CUDA (12.4 or 12.1); fallback to CPU\n",
    "import subprocess, re\n",
    "def _probe_cuda_tag():\n",
    "    try:\n",
    "        out = subprocess.check_output(['bash','-lc','nvcc --version || cat /usr/local/cuda/version.json || true'], text=True)\n",
    "        m = re.search(r'release (\\d+)\\.(\\d+)', out) or re.search(r'\"cuda\":\\s*\"(\\d+)\\.(\\d+)\"', out)\n",
    "        if m:\n",
    "            major, minor = m.groups()\n",
    "            ver = f\"{major}.{minor}\"\n",
    "        else:\n",
    "            ver = None\n",
    "    except Exception:\n",
    "        ver = None\n",
    "    if ver and ver.startswith('12.4'):\n",
    "        return 'cu124'\n",
    "    if ver and ver.startswith('12.1'):\n",
    "        return 'cu121'\n",
    "    # Default to cu121 in Colab\n",
    "    return 'cu121'\n",
    "\n",
    "_tag = _probe_cuda_tag()\n",
    "print('[torch] installing for', _tag)\n",
    "try:\n",
    "    get_ipython().system(\"pip -q install --upgrade --force-reinstall torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/%s\" % _tag)\n",
    "except Exception as e:\n",
    "    print('[warn] Torch install failed for', _tag, 'falling back to CPU:', e)\n",
    "    get_ipython().system(\"pip -q install --upgrade --force-reinstall torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cpu\")\n",
    "\n",
    "# 3) Core deps\n",
    "!pip -q install 'numpy<2.1,>=1.24'\n",
    "!pip -q install shapely timm opencv-python pycocotools addict yacs requests pillow huggingface_hub\n",
    "\n",
    "# 4) SAM and SAM2\n",
    "!pip -q install git+https://github.com/facebookresearch/segment-anything.git\n",
    "!pip -q install git+https://github.com/facebookresearch/segment-anything-2.git\n",
    "\n",
    "# 5) GroundingDINO — pip first, fallback to source build\n",
    "import sys, os, subprocess, importlib\n",
    "\n",
    "print('[GroundingDINO] installing via pip…')\n",
    "!pip -q install \"git+https://github.com/IDEA-Research/GroundingDINO.git\"\n",
    "\n",
    "try:\n",
    "    importlib.import_module('groundingdino')\n",
    "    print('✅ GroundingDINO pip import OK')\n",
    "except ImportError:\n",
    "    print('⚠️ pip install failed, building from source…')\n",
    "    if '/content/GroundingDINO' not in sys.path:\n",
    "        sys.path.append('/content/GroundingDINO')\n",
    "    !rm -rf /content/GroundingDINO\n",
    "    !git clone --depth 1 https://github.com/IDEA-Research/GroundingDINO.git /content/GroundingDINO\n",
    "    # Build C++ extensions\n",
    "    try:\n",
    "        subprocess.check_call(['bash','-lc','sudo apt-get -q update && sudo apt-get -q install -y ninja-build'])\n",
    "    except Exception as e:\n",
    "        print('[warn] apt-get ninja-build failed:', e)\n",
    "    build_dir = '/content/GroundingDINO'\n",
    "    orig = os.getcwd()\n",
    "    os.chdir(build_dir)\n",
    "    try:\n",
    "        res = subprocess.run([sys.executable, 'setup.py', 'build_ext', '--inplace'], capture_output=True, text=True)\n",
    "        if res.returncode != 0:\n",
    "            print('🟥 build_ext failed:\n",
    "', res.stderr)\n",
    "        else:\n",
    "            print('✅ build_ext ok')\n",
    "    finally:\n",
    "        os.chdir(orig)\n",
    "    try:\n",
    "        if 'groundingdino' in sys.modules:\n",
    "            importlib.reload(sys.modules['groundingdino'])\n",
    "        else:\n",
    "            importlib.import_module('groundingdino')\n",
    "        print('✅ GroundingDINO import OK after source build')\n",
    "    except Exception as e:\n",
    "        print('🟥 GroundingDINO import still failing:', e)\n",
    "\n",
    "# 6) Final checks\n",
    "try:\n",
    "    import torch, numpy\n",
    "    print(f\"[versions] torch: {torch.__version__}, numpy: {numpy.__version__}\")\n",
    "    from groundingdino.util.inference import Model\n",
    "    print('✅ GroundingDINO.Model available')\n",
    "    import segment_anything as _sam\n",
    "    print('✅ SAM v1 import OK')\n",
    "    try:\n",
    "        import sam2 as _sam2\n",
    "        print('✅ SAM2 import OK')\n",
    "    except Exception as e:\n",
    "        print('[warn] SAM2 import failed:', e)\n",
    "    print('CUDA available:', torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print('🟥 Final checks failed:', e)\n",
    "\n",
    "# 7) Harmonize\n",
    "!pip -q uninstall -y xformers || true\n",
    "!pip -q install -U 'numpy<2.1,>=1.24' typing_extensions>=4.14.0 filelock>=3.15 gcsfs==2025.3.0 fsspec==2025.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3faa9ae",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3faa9ae",
    "outputId": "2a3cb2e7-03bd-48c7-ce78-215852e6ecac"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch: 2.5.1+cu121 cuda: 12.1 available: True\n",
      "torchvision: 0.20.1+cu121\n",
      "[OK] torchvision.ops on CUDA\n",
      "GroundingDINO _C present: True\n",
      "[SELF-CHECK] req= auto  cuda_avail= True  tv_ops_ok= True  dino_ops_ok= True\n"
     ]
    }
   ],
   "source": [
    "#@title CUDA and C++ ops self-check\n",
    "import warnings, importlib, torch, torchvision, os\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "req = (DEVICE.lower() if 'DEVICE' in globals() else 'auto')\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "print('torch:', torch.__version__, 'cuda:', torch.version.cuda, 'available:', cuda_avail)\n",
    "print('torchvision:', torchvision.__version__)\n",
    "tv_ops_ok=False\n",
    "if cuda_avail:\n",
    "  try:\n",
    "    x=torch.rand(256,4,device='cuda'); y=torch.rand(256,4,device='cuda')\n",
    "    from torchvision.ops import box_iou\n",
    "    _=box_iou(x,y); tv_ops_ok=True; print('[OK] torchvision.ops on CUDA')\n",
    "  except Exception as e:\n",
    "    print('[WARN] torchvision.ops CUDA failed:', e)\n",
    "try:\n",
    "  m=importlib.import_module('groundingdino.models.GroundingDINO.ms_deform_attn')\n",
    "  dino_ops_ok=bool(getattr(m,'_C', None))\n",
    "  print('GroundingDINO _C present:', dino_ops_ok)\n",
    "except Exception as e:\n",
    "  dino_ops_ok=False; print('[WARN] GroundingDINO C++ ops import failed:', e)\n",
    "# Hard assertions when DEVICE='cuda'\n",
    "if req=='cuda':\n",
    "  assert cuda_avail, 'CUDA requested but not available'\n",
    "  assert tv_ops_ok, 'torchvision CUDA ops unavailable'\n",
    "  assert dino_ops_ok, 'GroundingDINO C++ ops (_C) not built'\n",
    "print('[SELF-CHECK] req=', req, ' cuda_avail=', cuda_avail, ' tv_ops_ok=', tv_ops_ok, ' dino_ops_ok=', dino_ops_ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79d75b85",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399,
     "referenced_widgets": [
      "56b1544659744dd78e36d9186d322332",
      "3fde95c8afe846c6b1404d94ec803802",
      "fabbd597d27c4606841512128409a260",
      "7aa7ff34954b4849930a7bf4b1c80eb3",
      "790fe38e7e014b1984fc2bf08eeadaed",
      "32524ec5b9764985ae245f4a7b5bad24",
      "f1a1afda6b7841e0bd951341277b7abf",
      "5952e31fe991400b80ba146e0ffc66d4",
      "79d1b35c8d154a218e66584cd2659009",
      "b5562aeaf5ce4518a00f8b3c6b603c1c",
      "6f61f45cee634ee0934d818cc0f392a1"
     ]
    },
    "id": "79d75b85",
    "outputId": "995d353b-7f40-4e16-bc63-703e3111dcc0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading GroundingDINO weights (robust)...\n",
      "[DINO] using GCS mirror\n",
      "Downloading SAM ViT-H weights (robust)...\n",
      "[SAM] using GCS mirror\n",
      "Downloading SAM2 Hiera Large weights (robust)...\n",
      "[warn] copy SAM2 from GCS mirror failed: [Errno 2] No such file or directory: '/content/models/sam2/sam2_hiera_large.pt'\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "sam2_hiera_large.pt:   0%|          | 0.00/898M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56b1544659744dd78e36d9186d322332"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[SAM2] using HF Hub\n",
      "[DINO] size= 693997677\n",
      "GROUNDING_MODEL = /content/models/groundingdino/groundingdino_swint_ogc.pth\n",
      "[SAM2] size= 897952466\n",
      "[SAM] size= 2564550879\n",
      "SAM_MODEL       = /content/models/sam/sam_vit_h_4b8939.pth\n"
     ]
    }
   ],
   "source": [
    "#@title Download/Resolve Model Weights\n",
    "#@title Download/Resolve Model Weights\n",
    "# (Optional) Download model weights if not present\n",
    "import os, pathlib, shutil, subprocess\n",
    "from typing import Optional\n",
    "pathlib.Path('/content/models/groundingdino').mkdir(parents=True, exist_ok=True)\n",
    "pathlib.Path('/content/models/sam').mkdir(parents=True, exist_ok=True)\n",
    "GROUNDING_MODEL = '/content/models/groundingdino/groundingdino_swint_ogc.pth'\n",
    "SAM_MODEL = '/content/models/sam/sam_vit_h_4b8939.pth'\n",
    "GROUNDING_URL = 'https://github.com/IDEA-Research/GroundingDINO/releases/download/0.1.0/groundingdino_swint_ogc.pth'\n",
    "SAM_URL = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\n",
    "SAM2_MODEL = '/content/models/sam2/sam2_hiera_large.pt'\n",
    "SAM2_URL = 'https://huggingface.co/facebook/sam2-hiera-large/resolve/main/sam2_hiera_large.pt'\n",
    "# Try to read HF token from Colab Keys or env\n",
    "HF_TOKEN = os.getenv('HF_TOKEN', '')\n",
    "try:\n",
    "  from google.colab import userdata as _ud\n",
    "  HF_TOKEN = _ud.get('HF_TOKEN') or HF_TOKEN\n",
    "except Exception:\n",
    "  pass\n",
    "def _file_ok(p: str, min_size: int) -> bool:\n",
    "  try:\n",
    "    return os.path.exists(p) and os.path.getsize(p) >= min_size\n",
    "  except Exception:\n",
    "    return False\n",
    "def _try_torch_load(p: str) -> bool:\n",
    "  try:\n",
    "    import torch\n",
    "    torch.load(p, map_location='cpu')\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print('[warn] torch.load failed:', e)\n",
    "    return False\n",
    "def _hf_download(repo_id: str, filename: str, dest: str) -> bool:\n",
    "  try:\n",
    "    from huggingface_hub import hf_hub_download, login\n",
    "    if HF_TOKEN:\n",
    "      try:\n",
    "        login(token=HF_TOKEN)\n",
    "      except Exception as e:\n",
    "        print('[warn] HF login failed:', e)\n",
    "    ckpt = hf_hub_download(repo_id=repo_id, filename=filename, local_dir=os.path.dirname(dest), local_dir_use_symlinks=False, token=HF_TOKEN or None)\n",
    "    if ckpt != dest:\n",
    "      shutil.copy2(ckpt, dest)\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print('[warn] HF download failed:', e)\n",
    "    return False\n",
    "def _curl(url: str, dest: str, min_size: int) -> bool:\n",
    "  cmd = f\"curl -L --fail --retry 5 --retry-all-errors -o '{dest}.tmp' '{url}'\"\n",
    "  rc = subprocess.call(cmd, shell=True)\n",
    "  if rc == 0 and _file_ok(dest + '.tmp', min_size):\n",
    "    shutil.move(dest + '.tmp', dest)\n",
    "    return True\n",
    "  else:\n",
    "    print('[warn] curl download insufficient or failed:', rc)\n",
    "    try:\n",
    "      os.remove(dest + '.tmp')\n",
    "    except Exception:\n",
    "      pass\n",
    "    return False\n",
    "# GroundedDINO (expect ~0.9GB)\n",
    "MIN_DINO = 600_000_000\n",
    "need_dino = (not _file_ok(GROUNDING_MODEL, MIN_DINO)) or (not _try_torch_load(GROUNDING_MODEL))\n",
    "if need_dino:\n",
    "  print('Downloading GroundingDINO weights (robust)...')\n",
    "  try:\n",
    "    os.remove(GROUNDING_MODEL)\n",
    "  except Exception:\n",
    "    pass\n",
    "  # 1) Try GCS mirror if mounted\n",
    "  gcs_mirror = '/content/artifacts/models/groundingdino/groundingdino_swint_ogc.pth'\n",
    "  ok = _file_ok(gcs_mirror, MIN_DINO)\n",
    "  if ok:\n",
    "    try:\n",
    "      shutil.copy2(gcs_mirror, GROUNDING_MODEL)\n",
    "      print('[DINO] using GCS mirror')\n",
    "      ok = True\n",
    "    except Exception as e:\n",
    "      print('[warn] copy from GCS mirror failed:', e); ok = False\n",
    "  # 2) Try HF Hub (public repo)\n",
    "  if (not ok):\n",
    "    ok = _hf_download('ShilongLiu/GroundingDINO', 'groundingdino_swint_ogc.pth', GROUNDING_MODEL)\n",
    "    if ok: print('[DINO] using HF Hub')\n",
    "  # 3) Try GitHub release via curl\n",
    "  if (not ok) or (not _file_ok(GROUNDING_MODEL, MIN_DINO)):\n",
    "    ok = _curl(GROUNDING_URL, GROUNDING_MODEL, MIN_DINO)\n",
    "    if ok: print('[DINO] using curl URL')\n",
    "  # 4) Try gsutil from bucket path if accessible\n",
    "  if (not ok) or (not _file_ok(GROUNDING_MODEL, MIN_DINO)):\n",
    "    try:\n",
    "      rc = subprocess.call(f\"gsutil cp gs://pik-artifacts-dev/models/groundingdino/groundingdino_swint_ogc.pth '{GROUNDING_MODEL}'\", shell=True)\n",
    "      ok = (rc == 0) and _file_ok(GROUNDING_MODEL, MIN_DINO)\n",
    "      if ok: print('[DINO] using gsutil mirror')\n",
    "    except Exception as e:\n",
    "      print('[warn] gsutil mirror copy failed:', e)\n",
    "  if (not ok) or (not _file_ok(GROUNDING_MODEL, MIN_DINO)) or (not _try_torch_load(GROUNDING_MODEL)):\n",
    "    raise SystemExit('Failed to fetch a valid GroundingDINO checkpoint')\n",
    "# SAM (ViT-H is large; check size only)\n",
    "MIN_SAM = 1_000_000_000\n",
    "if not _file_ok(SAM_MODEL, MIN_SAM):\n",
    "  print('Downloading SAM ViT-H weights (robust)...')\n",
    "  # 1) Try GCS mirror if mounted\n",
    "  sam_gcs_mirror = '/content/artifacts/models/sam/sam_vit_h_4b8939.pth'\n",
    "  ok = _file_ok(sam_gcs_mirror, MIN_SAM)\n",
    "  if ok:\n",
    "    try:\n",
    "      shutil.copy2(sam_gcs_mirror, SAM_MODEL)\n",
    "      print('[SAM] using GCS mirror')\n",
    "      ok = True\n",
    "    except Exception as e:\n",
    "      print('[warn] copy SAM from GCS mirror failed:', e); ok = False\n",
    "  # 2) Try HF Hub\n",
    "  if (not ok):\n",
    "    ok = _hf_download('facebook/sam', 'sam_vit_h_4b8939.pth', SAM_MODEL)\n",
    "    if ok: print('[SAM] using HF Hub')\n",
    "  # 3) Try official URL via curl\n",
    "  if (not ok) or (not _file_ok(SAM_MODEL, MIN_SAM)):\n",
    "    ok = _curl(SAM_URL, SAM_MODEL, MIN_SAM)\n",
    "    if ok: print('[SAM] using curl URL')\n",
    "  # 4) Try gsutil mirror from bucket\n",
    "  if (not ok) or (not _file_ok(SAM_MODEL, MIN_SAM)):\n",
    "    try:\n",
    "      rc = subprocess.call(f\"gsutil cp gs://pik-artifacts-dev/models/sam/sam_vit_h_4b8939.pth '{SAM_MODEL}'\", shell=True)\n",
    "      ok = (rc == 0) and _file_ok(SAM_MODEL, MIN_SAM)\n",
    "      if ok: print('[SAM] using gsutil mirror')\n",
    "    except Exception as e:\n",
    "      print('[warn] gsutil SAM mirror copy failed:', e)\n",
    "  if (not ok) or (not _file_ok(SAM_MODEL, MIN_SAM)):\n",
    "    raise SystemExit('Failed to fetch SAM ViT-H checkpoint')\n",
    "# SAM2 (Hiera Large)\n",
    "MIN_SAM2 = 700_000_000\n",
    "if not _file_ok(SAM2_MODEL, MIN_SAM2):\n",
    "  print('Downloading SAM2 Hiera Large weights (robust)...')\n",
    "  # 1) Try GCS mirror if mounted\n",
    "  sam2_gcs_mirror = '/content/artifacts/models/sam2/sam2_hiera_large.pt'\n",
    "  ok = _file_ok(sam2_gcs_mirror, MIN_SAM2)\n",
    "  if ok:\n",
    "    try:\n",
    "      shutil.copy2(sam2_gcs_mirror, SAM2_MODEL)\n",
    "      print('[SAM2] using GCS mirror')\n",
    "      ok = True\n",
    "    except Exception as e:\n",
    "      print('[warn] copy SAM2 from GCS mirror failed:', e); ok = False\n",
    "  # 2) Try HF Hub\n",
    "  if (not ok):\n",
    "    ok = _hf_download('facebook/sam2-hiera-large', 'sam2_hiera_large.pt', SAM2_MODEL)\n",
    "    if ok: print('[SAM2] using HF Hub')\n",
    "  # 3) Try direct URL via curl\n",
    "  if (not ok) or (not _file_ok(SAM2_MODEL, MIN_SAM2)):\n",
    "    ok = _curl(SAM2_URL, SAM2_MODEL, MIN_SAM2)\n",
    "    if ok: print('[SAM2] using curl URL')\n",
    "  # 4) Try gsutil mirror from bucket\n",
    "  if (not ok) or (not _file_ok(SAM2_MODEL, MIN_SAM2)):\n",
    "    try:\n",
    "      rc = subprocess.call(f\"gsutil cp gs://pik-artifacts-dev/models/sam2/sam2_hiera_large.pt '{SAM2_MODEL}'\", shell=True)\n",
    "      ok = (rc == 0) and _file_ok(SAM2_MODEL, MIN_SAM2)\n",
    "      if ok: print('[SAM2] using gsutil mirror')\n",
    "    except Exception as e:\n",
    "      print('[warn] gsutil SAM2 mirror copy failed:', e)\n",
    "  if (not ok) or (not _file_ok(SAM2_MODEL, MIN_SAM2)):\n",
    "    raise SystemExit('Failed to fetch SAM2 Hiera Large checkpoint')\n",
    "import os as _os; print('[DINO] size=', _os.path.getsize(GROUNDING_MODEL)); print('GROUNDING_MODEL =', GROUNDING_MODEL)\n",
    "import os as _os; print('[SAM2] size=', _os.path.getsize(SAM2_MODEL))\n",
    "import os as _os; print('[SAM] size=', _os.path.getsize(SAM_MODEL)); print('SAM_MODEL       =', SAM_MODEL)\n",
    "\n",
    "# Log weights info if logging enabled\n",
    "try:\n",
    "  import os as _os\n",
    "  WEIGHTS_INFO = {\n",
    "    'groundingdino': {'path': GROUNDING_MODEL, 'size': _os.path.getsize(GROUNDING_MODEL)},\n",
    "    'sam': {'path': SAM_MODEL, 'size': _os.path.getsize(SAM_MODEL)},\n",
    "    'sam2': ({'path': SAM2_MODEL, 'size': _os.path.getsize(SAM2_MODEL)} if _os.path.exists(SAM2_MODEL) else None),\n",
    "  }\n",
    "  if 'log_json' in globals(): log_json('weights.json', WEIGHTS_INFO)\n",
    "except Exception as e:\n",
    "  print('[LOG] weights info not recorded:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb7f44ca",
   "metadata": {
    "cellView": "form",
    "id": "cb7f44ca"
   },
   "outputs": [],
   "source": [
    "#@title Logging helpers (GCS)\n",
    "import os, sys, json, time, platform, socket, subprocess\n",
    "from pathlib import Path\n",
    "RUN_ID = time.strftime('%Y%m%d-%H%M%S') + (('-'+RUN_TAG.strip()) if ('RUN_TAG' in globals() and RUN_TAG.strip()) else '')\n",
    "LOCAL_LOG_ROOT = '/content/artifacts/colab_runs' if os.path.exists('/content/artifacts') else '/content/colab_runs'\n",
    "LOG_DIR = Path(LOCAL_LOG_ROOT)/RUN_ID\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PAGES_DIR = '/content/pages'\n",
    "PLAYBOOK_PDF = '/content/Playbook.pdf'\n",
    "DETECT_OUT = '/content/grounded_regions'\n",
    "\n",
    "def log_json(name, obj):\n",
    "  p = LOG_DIR/name if isinstance(name, Path) else LOG_DIR/str(name)\n",
    "  p.write_text(json.dumps(obj, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "def log_kv(key, val):\n",
    "  data = {}\n",
    "  p = LOG_DIR/'run.json'\n",
    "  if p.exists():\n",
    "    try: data = json.loads(p.read_text(encoding='utf-8'))\n",
    "    except Exception: data = {}\n",
    "  data[key]=val\n",
    "  p.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "env = { 'python': sys.version, 'platform': platform.platform(), 'hostname': socket.gethostname(), 'device_req': (DEVICE if 'DEVICE' in globals() else 'auto') }\n",
    "try:\n",
    "  import torch\n",
    "  env.update({'torch': torch.__version__, 'cuda': getattr(torch.version,'cuda',None), 'cuda_available': torch.cuda.is_available()})\n",
    "except Exception: pass\n",
    "log_json('env.json', env)\n",
    "def upload_logs():\n",
    "  if not ('REPORT_TO_GCS' in globals() and REPORT_TO_GCS):\n",
    "    print('[LOG] REPORT_TO_GCS disabled'); return\n",
    "  bucket = (GCS_BUCKET if 'GCS_BUCKET' in globals() else 'pik-artifacts-dev')\n",
    "  prefix = f'colab_runs/{RUN_ID}'\n",
    "  try:\n",
    "    from google.cloud import storage\n",
    "    client=storage.Client()\n",
    "    b=client.bucket(bucket)\n",
    "    for p in LOG_DIR.rglob('*'):\n",
    "      if p.is_file():\n",
    "        rel=str(p.relative_to(LOG_DIR)).replace('\\\\','/')\n",
    "        blob=b.blob(f'{prefix}/{rel}')\n",
    "        ctype='application/json' if p.suffix.lower()=='.json' else 'text/plain; charset=utf-8'\n",
    "        blob.content_type=ctype\n",
    "        blob.upload_from_filename(str(p))\n",
    "    print(f'[LOG] uploaded to gs://{bucket}/{prefix}')\n",
    "    return\n",
    "  except Exception as e:\n",
    "    print('[LOG] storage client failed, fallback to gsutil:', e)\n",
    "  # gsutil fallback\n",
    "  cmd=f\"gsutil -m cp -r '{LOG_DIR}' gs://{bucket}/colab_runs/\"\n",
    "  subprocess.run(['bash','-lc', cmd], check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d698d22",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d698d22",
    "outputId": "df7aab1d-b2b0-4f71-f184-fad96fe5e79b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[scan] Playbooks PDF : 3\n",
      "[render] 2023-06 - fastbreakOne - Expert Guide - Ecosystem Strategy  - English: pages=62 (dpi=150)\n",
      "[render] PIK - Expert Guide - Platform IT Architecture - Playbook - v11: pages=50 (dpi=150)\n",
      "[render] PIK 5-0 - Introduction - English: pages=49 (dpi=150)\n",
      "[scan] Frames PDF    : 29\n",
      "[render] L1 - Ecosystems Portfolio Map - v01: pages=1 (dpi=150)\n",
      "[render] L2 - Ecosystem Strategy Map: pages=1 (dpi=150)\n",
      "[render] L2 - Market Ecosystem Journey: pages=1 (dpi=150)\n",
      "[render] L3 - Platform Value Network Canvas: pages=1 (dpi=150)\n",
      "[render] PIK - Expert Guide - Platform IT Architecture - Assessment - v01: pages=1 (dpi=150)\n",
      "[progress] frames-pdf 5/29\n",
      "[render] PIK - Platform IT Architecture Canvas - Table View - v01: pages=1 (dpi=150)\n",
      "[render] PIK - Platform IT Architecture Canvases - v01: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Ecosystem Forces Scan - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Longtail Discovery - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - MVP - ENG: pages=1 (dpi=150)\n",
      "[progress] frames-pdf 10/29\n",
      "[render] PIK 5-0 - Market Sizing - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - NFX Reinforcement Engines - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Network Effects Stimulation - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform AARRR Funnel - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Business Model - ENG: pages=1 (dpi=150)\n",
      "[progress] frames-pdf 15/29\n",
      "[render] PIK 5-0 - Platform Crisis Response - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Evolution - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Experience - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Monetization Canvas - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Opportunity - ENG: pages=1 (dpi=150)\n",
      "[progress] frames-pdf 20/29\n",
      "[render] PIK 5-0 - Platform Stakeholder Persona - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Team - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform USP - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Platform Value Network Canvas - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Problem-Prioritization Matrix Canvas - ENG: pages=1 (dpi=150)\n",
      "[progress] frames-pdf 25/29\n",
      "[render] PIK 5-0 - Unfair Advantage - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - User Behaviour - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Value Chain Scan - ENG: pages=1 (dpi=150)\n",
      "[render] PIK 5-0 - Venture Governance - ENG: pages=1 (dpi=150)\n",
      "[scan] Frames PNG    : 25\n",
      "---- Summary ----\n",
      "Playbooks PDF                    : 3\n",
      "Frames PDF                       : 29\n",
      "Frames PNG                       : 25\n",
      "Pages rendered (playbooks+frames): 190\n",
      "Manifest appended                : 190\n",
      "Manifest                         : /content/full_run_manifest.jsonl\n",
      "Elapsed                          : 0.4s\n"
     ]
    }
   ],
   "source": [
    "#@title Source Full Run — playbooks (PDF) + frames (PDF+PNG) из /content/src_gcs\n",
    "# Источники (смонтированные бакеты):\n",
    "#   /content/src_gcs/playbooks   (PDF, рекурсивно **/*.pdf)\n",
    "#   /content/src_gcs/frames      (PDF+PNG, рекурсивно **/*.(pdf|png))\n",
    "# Выход:\n",
    "#   /content/pages_src/playbooks/<doc>/page-<n>.png\n",
    "#   /content/pages_src/frames/<doc>/page-<n>.png\n",
    "#   /content/full_run_manifest.jsonl        ({\"image\": \"<abs path>\"}, без дублей)\n",
    "\n",
    "import os, re, glob, json, subprocess, time\n",
    "from pathlib import Path\n",
    "\n",
    "# Смонтированный источник\n",
    "MOUNT_ROOT     = '/content/src_gcs'                # pik_source_bucket\n",
    "PLAYBOOKS_DIR  = f'{MOUNT_ROOT}/playbooks'         # PDF\n",
    "FRAMES_DIR     = f'{MOUNT_ROOT}/frames'            # PDF + PNG\n",
    "\n",
    "# Выходные пути\n",
    "OUT_PAGES_ROOT = '/content/pages_src'\n",
    "OUT_PAGES_PB   = f'{OUT_PAGES_ROOT}/playbooks'\n",
    "OUT_PAGES_FR   = f'{OUT_PAGES_ROOT}/frames'\n",
    "MANIFEST       = '/content/full_run_manifest.jsonl'\n",
    "\n",
    "# Параметры рендера\n",
    "DPI = 150\n",
    "RENDER_EXISTING = False        # перерисовывать существующие page-*.png?\n",
    "MAX_PAGES_PER_DOC = 0          # 0 = все страницы\n",
    "\n",
    "# Предусловия: playbooks/frames должны существовать\n",
    "for p in (PLAYBOOKS_DIR, FRAMES_DIR):\n",
    "    if not Path(p).exists():\n",
    "        raise SystemExit(f\"[fatal] Не найден путь: {p}. Проверь монтирование GCS.\")\n",
    "\n",
    "# Проверка poppler (pdftoppm/pdfinfo)\n",
    "def _ensure_poppler():\n",
    "    from shutil import which\n",
    "    missing = [x for x in ('pdftoppm','pdfinfo') if which(x) is None]\n",
    "    if missing:\n",
    "        raise SystemExit(f\"[fatal] Не найдены утилиты {missing}. Установи poppler-utils: !apt-get -q update && apt-get -q install -y poppler-utils\")\n",
    "_ensure_poppler()\n",
    "\n",
    "def _pdf_pages(pdf_path: str) -> int:\n",
    "    try:\n",
    "        out = subprocess.check_output(['pdfinfo', pdf_path], text=True)\n",
    "        m = re.search(r'^Pages:\\s+(\\d+)', out, re.M)\n",
    "        return int(m.group(1)) if m else 0\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def _render_doc(pdf: str, out_root: str, dpi: int = 150, limit: int = 0, rerender: bool = False) -> int:\n",
    "    \"\"\"Рендер одного PDF в out_root/<doc>/page-*.png. Возвращает кол-во страниц (написанных или пропущенных).\"\"\"\n",
    "    name = Path(pdf).stem\n",
    "    out_dir = Path(out_root) / name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    n = _pdf_pages(pdf)\n",
    "    if n <= 0:\n",
    "        print(f\"[warn] Не удалось определить число страниц: {pdf}. Пропуск.\")\n",
    "        return 0\n",
    "    if limit and limit > 0:\n",
    "        n = min(n, limit)\n",
    "    print(f\"[render] {name}: pages={n} (dpi={dpi})\")\n",
    "\n",
    "    done = 0\n",
    "    for p in range(1, n + 1):\n",
    "        png = out_dir / f'page-{p}.png'\n",
    "        if png.exists() and not rerender:\n",
    "            done += 1\n",
    "            continue\n",
    "        cmd = ['pdftoppm','-f',str(p),'-l',str(p),'-png','-singlefile','-r',str(dpi), pdf, str(png.with_suffix(''))]\n",
    "        subprocess.run(cmd, check=True)\n",
    "        done += 1\n",
    "        if (done % 10) == 0:\n",
    "            print(f\"  .. {done}/{n}\")\n",
    "    return done\n",
    "\n",
    "def _load_manifest_set(path: str) -> set:\n",
    "    s = set()\n",
    "    mp = Path(path)\n",
    "    if mp.exists():\n",
    "        with mp.open('r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line=line.strip()\n",
    "                if not line: continue\n",
    "                try:\n",
    "                    s.add(json.loads(line)['image'])\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return s\n",
    "\n",
    "# Основной поток\n",
    "Path(OUT_PAGES_PB).mkdir(parents=True, exist_ok=True)\n",
    "Path(OUT_PAGES_FR).mkdir(parents=True, exist_ok=True)\n",
    "t0 = time.time()\n",
    "\n",
    "# 1) PDF: playbooks → render\n",
    "pdfs_pb = sorted(glob.glob(os.path.join(PLAYBOOKS_DIR, '**', '*.pdf'), recursive=True))\n",
    "print('[scan] Playbooks PDF :', len(pdfs_pb))\n",
    "pages_written_pb = 0\n",
    "for i, pdf in enumerate(pdfs_pb, start=1):\n",
    "    try:\n",
    "        pages_written_pb += _render_doc(pdf, OUT_PAGES_PB, dpi=DPI, limit=MAX_PAGES_PER_DOC, rerender=RENDER_EXISTING)\n",
    "    except Exception as e:\n",
    "        print(f\"[error] playbooks pdf fail {pdf}:\", e)\n",
    "    if (i % 5) == 0:\n",
    "        print(f\"[progress] playbooks {i}/{len(pdfs_pb)}\")\n",
    "\n",
    "# 2) PDF: frames → render\n",
    "pdfs_fr = sorted(glob.glob(os.path.join(FRAMES_DIR, '**', '*.pdf'), recursive=True))\n",
    "print('[scan] Frames PDF    :', len(pdfs_fr))\n",
    "pages_written_fr = 0\n",
    "for i, pdf in enumerate(pdfs_fr, start=1):\n",
    "    try:\n",
    "        pages_written_fr += _render_doc(pdf, OUT_PAGES_FR, dpi=DPI, limit=MAX_PAGES_PER_DOC, rerender=RENDER_EXISTING)\n",
    "    except Exception as e:\n",
    "        print(f\"[error] frames pdf fail {pdf}:\", e)\n",
    "    if (i % 5) == 0:\n",
    "        print(f\"[progress] frames-pdf {i}/{len(pdfs_fr)}\")\n",
    "\n",
    "# 3) PNG: frames → просто собрать\n",
    "frames_png = sorted(glob.glob(os.path.join(FRAMES_DIR, '**', '*.png'), recursive=True))\n",
    "print('[scan] Frames PNG    :', len(frames_png))\n",
    "\n",
    "# 4) Обновить манифест (без дублей)\n",
    "existing = _load_manifest_set(MANIFEST)\n",
    "added = 0\n",
    "with Path(MANIFEST).open('a', encoding='utf-8') as f:\n",
    "    # Страницы из playbooks\n",
    "    page_pngs_pb = sorted(glob.glob(os.path.join(OUT_PAGES_PB, '**', 'page-*.png'), recursive=True))\n",
    "    for im in page_pngs_pb:\n",
    "        if im not in existing:\n",
    "            f.write(json.dumps({'image': im}, ensure_ascii=False) + '\\n')\n",
    "            existing.add(im); added += 1\n",
    "    # Страницы из frames (PDF)\n",
    "    page_pngs_fr = sorted(glob.glob(os.path.join(OUT_PAGES_FR, '**', 'page-*.png'), recursive=True))\n",
    "    for im in page_pngs_fr:\n",
    "        if im not in existing:\n",
    "            f.write(json.dumps({'image': im}, ensure_ascii=False) + '\\n')\n",
    "            existing.add(im); added += 1\n",
    "    # Исходные кадры PNG\n",
    "    for im in frames_png:\n",
    "        if im not in existing:\n",
    "            f.write(json.dumps({'image': im}, ensure_ascii=False) + '\\n')\n",
    "            existing.add(im); added += 1\n",
    "\n",
    "t1 = time.time()\n",
    "print('---- Summary ----')\n",
    "print('Playbooks PDF                    :', len(pdfs_pb))\n",
    "print('Frames PDF                       :', len(pdfs_fr))\n",
    "print('Frames PNG                       :', len(frames_png))\n",
    "print('Pages rendered (playbooks+frames):', pages_written_pb + pages_written_fr)\n",
    "print('Manifest appended                :', added)\n",
    "print('Manifest                         :', MANIFEST)\n",
    "print(f'Elapsed                          : {t1 - t0:.1f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Подготовить список изображений для Боевого режима из манифеста\n",
    "import json\n",
    "MANIFEST = '/content/full_run_manifest.jsonl'\n",
    "images = [json.loads(l)['image'] for l in open(MANIFEST, 'r', encoding='utf-8') if l.strip()]\n",
    "IMAGES = images  # если боевой режим ждёт именно IMAGES\n",
    "print('Images to process:', len(images))\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lSaJ4sWccUy",
    "outputId": "6478110e-2cfb-4f24-b1bc-325c64fc0c98"
   },
   "id": "3lSaJ4sWccUy",
   "execution_count": 42,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Images to process: 376\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#@title Define detect_image_to_regions (SAM/SAM2 — real regions)\n",
    "import os, json, base64, io\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Параметры весов\n",
    "SAM2_MODEL = os.environ.get('SAM2_MODEL', '/content/models/sam2/sam2_hiera_large.pt')\n",
    "SAM_V1_CANDIDATES = [\n",
    "    os.environ.get('SAM_MODEL', ''),                     # приоритет — если задан\n",
    "    '/content/models/sam/sam_vit_b_01ec64.pth',          # компактный (~370MB)\n",
    "    '/content/models/sam/sam_vit_l_0b3195.pth',\n",
    "    '/content/models/sam/sam_vit_h_4b8939.pth',\n",
    "]\n",
    "AUTO_DOWNLOAD_SAM_V1 = False  # поставь True, если нет весов SAM v1, и можно подождать скачивание (~370MB для ViT-B)\n",
    "\n",
    "def _b64_png(img: Image.Image) -> str:\n",
    "    buf = io.BytesIO()\n",
    "    img.save(buf, format='PNG')\n",
    "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "\n",
    "def _bbox_from_mask(mask: np.ndarray) -> tuple:\n",
    "    # mask: HxW boolean/0-1\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(xs) == 0 or len(ys) == 0:\n",
    "        return (0, 0, -1, -1)\n",
    "    x0, x1 = int(xs.min()), int(xs.max())\n",
    "    y0, y1 = int(ys.min()), int(ys.max())\n",
    "    return (x0, y0, x1 - x0 + 1, y1 - y0 + 1)\n",
    "\n",
    "def _try_sam2():\n",
    "    try:\n",
    "        import torch\n",
    "        from sam2.build_sam import build_sam2\n",
    "        from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "        if not (SAM2_MODEL and Path(SAM2_MODEL).exists()):\n",
    "            return None\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        model = build_sam2('sam2_hiera_large', SAM2_MODEL, device=device)\n",
    "        predictor = SAM2ImagePredictor(model)\n",
    "        return ('sam2', predictor, device)\n",
    "    except Exception as e:\n",
    "        print('[warn] SAM2 init failed:', e)\n",
    "        return None\n",
    "\n",
    "def _try_sam_v1():\n",
    "    try:\n",
    "        import torch\n",
    "        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "        ckpt = next((p for p in SAM_V1_CANDIDATES if p and Path(p).exists()), None)\n",
    "        if ckpt is None and AUTO_DOWNLOAD_SAM_V1:\n",
    "            # Скачиваем лёгкий ViT-B\n",
    "            url = 'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth'\n",
    "            Path('/content/models/sam').mkdir(parents=True, exist_ok=True)\n",
    "            dst = '/content/models/sam/sam_vit_b_01ec64.pth'\n",
    "            import subprocess\n",
    "            print('[sam] downloading:', url)\n",
    "            subprocess.run(['bash','-lc', f\"curl -L '{url}' -o '{dst}'\"], check=True)\n",
    "            ckpt = dst\n",
    "        if ckpt is None:\n",
    "            return None\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        arch = 'vit_b' if 'vit_b' in ckpt else ('vit_l' if 'vit_l' in ckpt else 'vit_h')\n",
    "        sam = sam_model_registry[arch](checkpoint=ckpt).to(device)\n",
    "        mask_gen = SamAutomaticMaskGenerator(\n",
    "            sam,\n",
    "            points_per_side=16,\n",
    "            pred_iou_thresh=0.88,\n",
    "            stability_score_thresh=0.92,\n",
    "            min_mask_region_area=4000,\n",
    "        )\n",
    "        return ('sam_v1', mask_gen, device)\n",
    "    except Exception as e:\n",
    "        print('[warn] SAM v1 init failed:', e)\n",
    "        return None\n",
    "\n",
    "# Ленивая инициализация (один раз на сессию)\n",
    "_DET_BACKEND = {'kind': None, 'obj': None, 'device': None}\n",
    "\n",
    "def _ensure_backend():\n",
    "    if _DET_BACKEND['kind'] is not None:\n",
    "        return _DET_BACKEND\n",
    "    b = _try_sam2()\n",
    "    if b is None:\n",
    "        b = _try_sam_v1()\n",
    "    if b is None:\n",
    "        raise SystemExit('[fatal] No SAM/SAM2 backend available. Provide weights or enable AUTO_DOWNLOAD_SAM_V1=True.')\n",
    "    _DET_BACKEND['kind'], _DET_BACKEND['obj'], _DET_BACKEND['device'] = b\n",
    "    print('[detector] using', _DET_BACKEND['kind'], 'on', _DET_BACKEND['device'])\n",
    "    return _DET_BACKEND\n",
    "\n",
    "def detect_image_to_regions(image_path: str, out_dir: str, max_regions: int = 8):\n",
    "    \"\"\"\n",
    "    Реальная детекция регионов:\n",
    "    - если доступен SAM2 — используем его (grid‑prompt + bbox из масок);\n",
    "    - иначе SAM v1 (Automatic Mask Generator).\n",
    "    Сохраняет region-*.json (+ preview region-*.png) в out_dir/regions.\n",
    "    \"\"\"\n",
    "    be = _ensure_backend()\n",
    "    kind, obj, device = be['kind'], be['obj'], be['device']\n",
    "\n",
    "    img = Image.open(image_path).convert('RGB')\n",
    "    img_np = np.array(img)\n",
    "\n",
    "    regions_dir = Path(out_dir) / 'regions'\n",
    "    regions_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    masks = []\n",
    "    if kind == 'sam2':\n",
    "        # Простейшая сетка точек для инициализации (быстро и надёжно)\n",
    "        from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "        obj.set_image(img_np)\n",
    "        H, W = img_np.shape[:2]\n",
    "        steps = 5\n",
    "        xs = np.linspace(W*0.1, W*0.9, steps).astype(int)\n",
    "        ys = np.linspace(H*0.1, H*0.9, steps).astype(int)\n",
    "        for y in ys:\n",
    "            for x in xs:\n",
    "                masks_pred, scores, _ = obj.predict(point_coords=np.array([[x,y]]), point_labels=np.array([1]), multimask_output=True)\n",
    "                # masks_pred: (k, H, W) <- boolean\n",
    "                for m in masks_pred:\n",
    "                    masks.append(m.astype(np.uint8))\n",
    "    else:\n",
    "        # SAM v1 — автоматический генератор масок\n",
    "        masks_gen = obj.generate(img_np)  # list of dicts: {'segmentation', 'bbox', ...}\n",
    "        for m in masks_gen:\n",
    "            seg = m.get('segmentation')\n",
    "            if seg is not None:\n",
    "                masks.append(seg.astype(np.uint8))\n",
    "\n",
    "    # Отсортируем по площади и возьмём топ‑N\n",
    "    scored = []\n",
    "    for m in masks:\n",
    "        area = int((m > 0).sum())\n",
    "        if area > 1000:\n",
    "            scored.append((area, m))\n",
    "    scored.sort(reverse=True)\n",
    "    picked = [m for _, m in scored[:max_regions]]\n",
    "\n",
    "    # Запись регионов\n",
    "    written = 0\n",
    "    for i, m in enumerate(picked, start=1):\n",
    "        x, y, w, h = _bbox_from_mask(m)\n",
    "        if w <= 0 or h <= 0:\n",
    "            continue\n",
    "        crop = img.crop((x, y, x + w, y + h))\n",
    "        b64 = _b64_png(crop)\n",
    "        (regions_dir / f'region-{i}.json').write_text(\n",
    "            json.dumps({'bbox': {'x': x, 'y': y, 'w': w, 'h': h}, 'text': '', 'image_b64': b64}, ensure_ascii=False),\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        crop.save(regions_dir / f'region-{i}.png', 'PNG')\n",
    "        written += 1\n",
    "\n",
    "    if written == 0:\n",
    "        # В крайнем случае — хотя бы один регион (всё изображение)\n",
    "        x, y, w, h = 0, 0, img.width, img.height\n",
    "        b64 = _b64_png(img)\n",
    "        (regions_dir / f'region-1.json').write_text(\n",
    "            json.dumps({'bbox': {'x': x, 'y': y, 'w': w, 'h': h}, 'text': '', 'image_b64': b64}, ensure_ascii=False),\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        img.save(regions_dir / f'region-1.png', 'PNG')\n",
    "        written = 1\n",
    "\n",
    "    print(f\"[detect] {Path(image_path).name}: regions={written} → {regions_dir}\")\n"
   ],
   "metadata": {
    "id": "6e7FewipkqlJ"
   },
   "id": "6e7FewipkqlJ",
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4be7c64",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4be7c64",
    "outputId": "3d146488-bc00-49c6-e42f-975db4a33929"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Selected device: cuda\n",
      "SAM v1 ready on cuda\n",
      "final text_encoder_type: bert-base-uncased\n",
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "Copying file:///content/grounded_regions/frames/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Ecosystem Forces Scan - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK - Platform IT Architecture Canvases - v01/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK - Platform IT Architecture Canvases - v01/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Ecosystem Forces Scan - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Longtail Discovery - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - MVP - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Market Sizing - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Market Sizing - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - MVP - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Longtail Discovery - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - NFX Reinforcement Engines - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Network Effects Stimulation - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Network Effects Stimulation - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - NFX Reinforcement Engines - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform AARRR Funnel - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform AARRR Funnel - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Business Model - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Business Model - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Crisis Response - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Crisis Response - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Evolution - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Experience - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Monetization Canvas - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Experience - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Opportunity - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Monetization Canvas - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Evolution - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Stakeholder Persona - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Team - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Stakeholder Persona - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform USP - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Value Network Canvas - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform USP - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Value Network Canvas - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Team - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Platform Opportunity - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Problem-Prioritization Matrix - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Problem-Prioritization Matrix - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Unfair Advantage - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Unfair Advantage - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - User Behaviour - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - User Behaviour - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Value Chain Scan - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Value Chain Scan - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Venture Governance - ENG/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/frames/PIK 5-0 - Venture Governance - ENG/regions/region-1.png [Content-Type=image/png]...\n",
      "Copying file:///content/grounded_regions/misc/regions/region-1.json [Content-Type=application/json]...\n",
      "Copying file:///content/grounded_regions/misc/regions/region-1.png [Content-Type=image/png]...\n",
      "/ [52/52 files][ 16.2 MiB/ 16.2 MiB] 100% Done                                  \n",
      "Operation completed over 52 objects/16.2 MiB.                                    \n",
      "\n",
      "WARNING: gsutil rsync uses hashes when modification time is not available at\n",
      "both the source and destination. Your crcmod installation isn't using the\n",
      "module's C extension, so checksumming will run very slowly. If this is your\n",
      "first rsync since updating gsutil, this rsync can take significantly longer than\n",
      "usual. For help installing the extension, please see \"gsutil help crcmod\".\n",
      "\n",
      "Building synchronization state...\n",
      "Starting synchronization...\n",
      "gs://pik-artifacts-dev/grounded_regions/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Expert Guide - Platform IT Architecture - Assessment - v01/regions/region-1.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-1.png\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-2.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvas - Table View - v01/regions/region-2.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-1.png\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-2.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK - Platform IT Architecture Canvases - v01/regions/region-2.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Ecosystem Forces Scan - ENG/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Ecosystem Forces Scan - ENG/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Ecosystem Forces Scan - ENG/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Ecosystem Forces Scan - ENG/regions/region-1.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Longtail Discovery - ENG/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Longtail Discovery - ENG/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Longtail Discovery - ENG/regions/region-1.json\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - Longtail Discovery - ENG/regions/region-1.png\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - MVP - ENG/:\n",
      "\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - MVP - ENG/regions/:\n",
      "gs://pik-artifacts-dev/grounded_regions/PIK 5-0 - MVP - ENG/regions/region-1.json\n",
      "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    }
   ],
   "source": [
    "# Боевой режим: GroundedDINO → SAM\n",
    "require_start()\n",
    "\n",
    "import os, json, pathlib, cv2, numpy as np, torch\n",
    "from groundingdino.util.inference import Model\n",
    "# SAM/SAM2 init with fallback and device control\n",
    "_req = (DEVICE.lower() if 'DEVICE' in globals() else 'auto')\n",
    "if _req == 'cuda' and not torch.cuda.is_available():\n",
    "  print('[warn] CUDA requested but not available; using CPU')\n",
    "  device = 'cpu'\n",
    "elif _req == 'cpu':\n",
    "  device = 'cpu'\n",
    "else:\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Selected device:', device)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "sam2_predictor = None\n",
    "if 'USE_SAM2' in globals() and USE_SAM2:\n",
    "  try:\n",
    "    from sam2.build_sam import build_sam2\n",
    "    from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "    sam2_model = build_sam2('sam2_hiera_large', SAM2_MODEL, device=device)\n",
    "    sam2_predictor = SAM2ImagePredictor(sam2_model)\n",
    "    print('SAM2 ready on', device)\n",
    "  except Exception as e:\n",
    "    print('SAM2 init failed, fallback to SAM v1:', e)\n",
    "    sam2_predictor = None\n",
    "if sam2_predictor is None:\n",
    "  from segment_anything import sam_model_registry, SamPredictor\n",
    "  print('SAM v1 ready on', device)\n",
    "CFG_PATH = '/content/GroundingDINO_SwinT_OGC.py'\n",
    "CFG_URL = 'https://raw.githubusercontent.com/IDEA-Research/GroundingDINO/main/groundingdino/config/GroundingDINO_SwinT_OGC.py'\n",
    "# Попытка скачать конфиг, если его нет\n",
    "import urllib.request, urllib.error\n",
    "def _download(url, path):\n",
    "  try:\n",
    "    urllib.request.urlretrieve(url, path)\n",
    "    return os.path.exists(path) and os.path.getsize(path) > 1000\n",
    "  except Exception:\n",
    "    return False\n",
    "if not os.path.exists(CFG_PATH):\n",
    "  ok = _download(CFG_URL, CFG_PATH)\n",
    "  if not ok:\n",
    "    try:\n",
    "      import groundingdino, os as _os\n",
    "      CFG_PATH = _os.path.join(_os.path.dirname(groundingdino.__file__), 'config', 'GroundingDINO_SwinT_OGC.py')\n",
    "      print('Using package config at', CFG_PATH)\n",
    "    except Exception as e:\n",
    "      raise FileNotFoundError('GroundingDINO config not found and download failed')\n",
    "# Sanity check on DINO checkpoint\n",
    "import torch\n",
    "try:\n",
    "  _ = torch.load(GROUNDING_MODEL, map_location='cpu')\n",
    "except Exception as e:\n",
    "  raise RuntimeError(f'GroundedDINO checkpoint invalid: {e}')\n",
    "gd_model = Model(model_config_path=CFG_PATH, model_checkpoint_path=GROUNDING_MODEL, device=device)\n",
    "def save_region(rdir, idx, img, xyxy):\n",
    "  x0,y0,x1,y1 = map(int, xyxy)\n",
    "  x0,y0 = max(0,x0), max(0,y0)\n",
    "  crop = img[y0:y1, x0:x1] if y1>y0 and x1>x0 else img\n",
    "  ok, buf = cv2.imencode('.png', crop)\n",
    "  if ok: (rdir/f'region-{idx}.png').write_bytes(buf.tobytes())\n",
    "  obj = { 'bbox': {'x':int(x0),'y':int(y0),'w':int(x1-x0),'h':int(y1-y0)}, 'text':'', 'image_b64':'' }\n",
    "  (rdir/f'region-{idx}.json').write_text(json.dumps(obj, ensure_ascii=False), encoding='utf-8')\n",
    "\n",
    "def ensure_dir(d):\n",
    "    pathlib.Path(d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def detect_one(image_path, out_root):\n",
    "  img = cv2.imread(image_path); assert img is not None, image_path\n",
    "  img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  H,W = img_rgb.shape[:2]\n",
    "  output = gd_model.predict_with_classes(image=img_rgb, classes=PROMPTS, box_threshold=BOX_THRESHOLD, text_threshold=TEXT_THRESHOLD)\n",
    "  print(f\"Output from predict_with_classes: {output}\") # Debugging line\n",
    "  boxes = output.xyxy # Access boxes from the Detections object\n",
    "  logits = output.confidence # Access confidence scores from the Detections object\n",
    "  phrases = [PROMPTS[class_id] for class_id in output.class_id] # Infer phrases from class_id and PROMPTS\n",
    "  bxs = []\n",
    "  for b in boxes:\n",
    "    b = np.asarray(b, dtype=float)\n",
    "    if b.max()<=1.01: x0,y0,x1,y1 = b[0]*W, b[1]*H, b[2]*W, b[3]*H\n",
    "    else: x0,y0,x1,y1 = b\n",
    "    bxs.append([x0,y0,x1,y1])\n",
    "  out = os.path.join(out_root, pathlib.Path(image_path).stem, 'regions'); ensure_dir(out)\n",
    "  for i,xyxy in enumerate(bxs[:TOPK], start=1): save_region(pathlib.Path(out), i, img, xyxy)\n",
    "# Страницы\n",
    "ensure_dir(DETECT_OUT)\n",
    "for p in PAGES:\n",
    "  detect_one(f'/content/pages/page-{p}.png', DETECT_OUT)\n",
    "# Фреймы\n",
    "ensure_dir('/content/grounded_frames') # Ensure directory exists\n",
    "for name in FRAME_NAMES:\n",
    "  f = f'/content/src_gcs/frames/{name}'\n",
    "  if os.path.exists(f): detect_one(f, '/content/grounded_frames')\n",
    "# Выгрузка\n",
    "!gsutil -m rsync -r /content/grounded_regions gs://pik-artifacts-dev/grounded_regions/\n",
    "!gsutil -m rsync -r /content/grounded_frames gs://pik-artifacts-dev/grounded_regions/\n",
    "!gsutil ls -r gs://pik-artifacts-dev/grounded_regions | head -n 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed3538c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ed3538c6",
    "outputId": "7568bac5-4d71-4a1b-a609-ef9e3765093e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[local] units: ['PIK - Expert Guide - Platform IT Architecture - Assessment - v01', 'PIK - Platform IT Architecture Canvas - Table View - v01', 'PIK - Platform IT Architecture Canvases - v01', 'PIK 5-0 - Ecosystem Forces Scan - ENG', 'PIK 5-0 - Longtail Discovery - ENG', 'PIK 5-0 - MVP - ENG', 'PIK 5-0 - Market Sizing - ENG', 'PIK 5-0 - NFX Reinforcement Engines - ENG'] ...\n",
      "[local] files: 348\n",
      "> using fuse: rsync -av '/content/grounded_regions/' '/content/artifacts/grounded_regions/'\n",
      "[done] fuse sync -> /content/artifacts/grounded_regions | files=368 | 8.5s\n",
      "[hint] Регионы доступны под префиксом: grounded_regions\n"
     ]
    }
   ],
   "source": [
    "#@title Upload Regions to GCS (grounded_regions)\n",
    "require_start()\n",
    "\n",
    "# Синхронизация локальных регионов в GCS: либо через смонтированный бакет (/content/artifacts),\n",
    "# либо через gsutil rsync. Идемпотентно.\n",
    "\n",
    "import os, glob, subprocess, time\n",
    "from pathlib import Path\n",
    "\n",
    "# Локальный корень детекции (сюда пишет “боевой режим”)\n",
    "LOCAL_REGIONS = '/content/grounded_regions'    # EXPECT: <unit>/regions/region-*.{json,png,...}\n",
    "DEST_PREFIX   = 'grounded_regions'             # базовый префикс в bakete\n",
    "RUN_TAG       = os.environ.get('RUN_TAG', '')  # например: '2025-09-16-1040' или 'full-run'\n",
    "GCS_BUCKET    = os.environ.get('GCS_BUCKET', 'pik-artifacts-dev')  # имя бакета\n",
    "\n",
    "# Смонтированный путь к бакету (если GCSFuse подключен): /content/artifacts -> gs://pik-artifacts-dev/\n",
    "MOUNTED_ARTIFACTS = '/content/artifacts'\n",
    "\n",
    "def _which(x: str) -> bool:\n",
    "    from shutil import which\n",
    "    return which(x) is not None\n",
    "\n",
    "def _count_local(root: str) -> int:\n",
    "    return len(glob.glob(os.path.join(root, '**', '*'), recursive=True))\n",
    "\n",
    "# Проверки\n",
    "if not Path(LOCAL_REGIONS).exists():\n",
    "    raise SystemExit(f\"[fatal] LOCAL_REGIONS не найден: {LOCAL_REGIONS}. Сначала запусти детекцию.\")\n",
    "\n",
    "# Соберём чуть статистики\n",
    "local_files = _count_local(LOCAL_REGIONS)\n",
    "units = sorted({Path(p).parts[-3] for p in glob.glob(os.path.join(LOCAL_REGIONS, '**', 'regions', 'region-*.json'), recursive=True)})\n",
    "print('[local] units:', units[:8], ('...' if len(units) > 8 else ''))\n",
    "print('[local] files:', local_files)\n",
    "\n",
    "# Назначение\n",
    "if RUN_TAG:\n",
    "    dst_suffix = f\"{DEST_PREFIX}/{RUN_TAG}\"\n",
    "else:\n",
    "    dst_suffix = DEST_PREFIX\n",
    "\n",
    "# Вариант 1: GCSFuse (копирование в смонтированный путь)\n",
    "mounted_root = Path(MOUNTED_ARTIFACTS)\n",
    "if mounted_root.exists():\n",
    "    dest_dir = mounted_root / dst_suffix\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # rsync внутри локальной FS — быстро и просто\n",
    "    # -a (архив), -v (verbose); заменит/удалит изменённые/удалённые при -r/-d\n",
    "    # оставим без -d (delete), чтобы не удалять предыдущие загрузки в подпапке\n",
    "    cmd = f\"rsync -av '{LOCAL_REGIONS}/' '{dest_dir}/'\"\n",
    "    print('> using fuse:', cmd)\n",
    "    t0 = time.time()\n",
    "    subprocess.run(['bash','-lc', cmd], check=True)\n",
    "    dt = time.time() - t0\n",
    "\n",
    "    # Быстрый подсчёт\n",
    "    remote_files = len(glob.glob(os.path.join(str(dest_dir), '**', '*'), recursive=True))\n",
    "    print(f\"[done] fuse sync -> {dest_dir} | files={remote_files} | {dt:.1f}s\")\n",
    "\n",
    "# Вариант 2: gsutil rsync (при отсутствии GCSFuse)\n",
    "else:\n",
    "    if not _which('gsutil'):\n",
    "        raise SystemExit(\"[fatal] Ни GCSFuse (/content/artifacts), ни gsutil не найдены. Нечем выгружать.\")\n",
    "    dest_uri = f\"gs://{GCS_BUCKET}/{dst_suffix}\"\n",
    "    # -m: мультипоточно; -r: рекурсивно; без -d, чтобы не удалять прошлое\n",
    "    cmd = f\"gsutil -m rsync -r '{LOCAL_REGIONS}' '{dest_uri}'\"\n",
    "    print('> using gsutil:', cmd)\n",
    "    t0 = time.time()\n",
    "    subprocess.run(['bash','-lc', cmd], check=True)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[done] gsutil rsync -> {dest_uri} | {dt:.1f}s\")\n",
    "\n",
    "print('[hint] Регионы доступны под префиксом:', dst_suffix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJlE0rWGvJRa",
   "metadata": {
    "id": "VJlE0rWGvJRa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d46c987f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d46c987f",
    "outputId": "035feb0d-363d-4eb7-ddd3-6746207645c3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[warn] cells.jsonl not found at /content/artifacts/colab_runs/20250916-165657/cells.jsonl — создаю пустой (запусти любую ячейку после логгера, чтобы появились записи).\n",
      "[log] run_info.json: /content/artifacts/colab_runs/20250916-165657/run_info.json\n",
      "> using fuse: rsync -av '/content/artifacts/colab_runs/20250916-165657/' '/content/artifacts/colab_runs/20250916-165657/'\n",
      "[done] fuse sync -> /content/artifacts/colab_runs/20250916-165657 | files=3 | 0.1s\n",
      "Remote path (fuse): /content/artifacts/colab_runs/20250916-165657\n",
      "[hint] LOG_DIR = /content/artifacts/colab_runs/20250916-165657\n"
     ]
    }
   ],
   "source": [
    "#@title Upload Cell Logs to GCS (robust: autodetect LOG_DIR, fuse/gsutil)\n",
    "require_start()\n",
    "import os, json, time, glob, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "# 0) Найти локальный LOG_DIR с cells.jsonl (а НЕ в /content/artifacts)\n",
    "def _pick_log_dir():\n",
    "    # 0.1 Явно заданная переменная\n",
    "    if 'LOG_DIR' in globals():\n",
    "        p = Path(globals()['LOG_DIR'])\n",
    "        if p.exists(): return p\n",
    "    # 0.2 RUN_ID из окружения или из ранее заданного\n",
    "    rid = os.environ.get('RUN_ID', None)\n",
    "    if rid:\n",
    "        p = Path('/content/colab_runs') / rid\n",
    "        if p.exists(): return p\n",
    "    # 0.3 Самый свежий подкаталог в /content/colab_runs\n",
    "    root = Path('/content/colab_runs')\n",
    "    if root.exists():\n",
    "        dirs = sorted([d for d in root.iterdir() if d.is_dir()],\n",
    "                      key=lambda d: d.stat().st_mtime, reverse=True)\n",
    "        if dirs: return dirs[0]\n",
    "    # 0.4 В крайнем случае — создаём новый\n",
    "    rid = time.strftime('%Y%m%d-%H%M%S')\n",
    "    p = Path('/content/colab_runs') / rid\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "LOG_DIR = _pick_log_dir()\n",
    "LOG_DIR = Path(LOG_DIR)\n",
    "cells_path = LOG_DIR / 'cells.jsonl'\n",
    "\n",
    "# 1) Если лог не появился — создадим пустой, чтобы не падать\n",
    "if not cells_path.exists():\n",
    "    print(f\"[warn] cells.jsonl not found at {cells_path} — создаю пустой (запусти любую ячейку после логгера, чтобы появились записи).\")\n",
    "    cells_path.write_text('', encoding='utf-8')\n",
    "\n",
    "# 2) Служебный run_info.json\n",
    "info = {\n",
    "    'run_id': LOG_DIR.name,\n",
    "    'notebook_version': globals().get('NOTEBOOK_VERSION', 'unknown'),\n",
    "    'notebook_updated': globals().get('NOTEBOOK_UPDATED', 'unknown'),\n",
    "    'timestamp_utc': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),\n",
    "    'log_dir': str(LOG_DIR),\n",
    "    'files_in_log_dir': len(list(LOG_DIR.glob('**/*'))),\n",
    "}\n",
    "info_path = LOG_DIR / 'run_info.json'\n",
    "info_path.write_text(json.dumps(info, ensure_ascii=False, indent=2), encoding='utf-8')\n",
    "print('[log] run_info.json:', info_path)\n",
    "\n",
    "def _which(name: str) -> bool:\n",
    "    from shutil import which\n",
    "    return which(name) is not None\n",
    "\n",
    "# 3) Назначение в бакете\n",
    "GCS_BUCKET  = os.environ.get('GCS_BUCKET', 'pik-artifacts-dev')\n",
    "DEST_PREFIX = os.environ.get('DEST_PREFIX', 'colab_runs')\n",
    "RUN_ID_STR  = LOG_DIR.name\n",
    "DEST_SUBDIR = f'{DEST_PREFIX}/{RUN_ID_STR}'\n",
    "\n",
    "# 4) Путь через FUSE (если смонтирован /content/artifacts)\n",
    "ARTIFACTS_MOUNT = Path('/content/artifacts')\n",
    "if ARTIFACTS_MOUNT.exists():\n",
    "    dst_dir = ARTIFACTS_MOUNT / DEST_SUBDIR\n",
    "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # rsync всего подкаталога (cells.jsonl + run_info.json + проч.)\n",
    "    cmd = f\"rsync -av '{LOG_DIR}/' '{dst_dir}/'\"\n",
    "    print('> using fuse:', cmd)\n",
    "    t0 = time.time()\n",
    "    subprocess.run(['bash','-lc', cmd], check=True)\n",
    "    dt = time.time() - t0\n",
    "    remote_count = len(list(dst_dir.glob('**/*')))\n",
    "    print(f\"[done] fuse sync -> {dst_dir} | files={remote_count} | {dt:.1f}s\")\n",
    "    print('Remote path (fuse):', dst_dir)\n",
    "else:\n",
    "    # 5) Фолбэк — gsutil\n",
    "    if not _which('gsutil'):\n",
    "        raise SystemExit('[fatal] Ни FUSE (/content/artifacts), ни gsutil не найдены — лог некуда выгрузить.')\n",
    "    dest_uri = f\"gs://{GCS_BUCKET}/{DEST_SUBDIR}\"\n",
    "    print('> using gsutil: upload log files')\n",
    "    t0 = time.time()\n",
    "    subprocess.run(['bash','-lc', f\"gsutil -h 'Content-Type:application/json' cp -r '{LOG_DIR}/cells.jsonl' '{dest_uri}/'\"], check=True)\n",
    "    subprocess.run(['bash','-lc', f\"gsutil -h 'Content-Type:application/json' cp -r '{LOG_DIR}/run_info.json' '{dest_uri}/'\"], check=True)\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[done] gsutil cp -> {dest_uri} | {dt:.1f}s\")\n",
    "    print('Remote path (gsutil):', dest_uri)\n",
    "\n",
    "print('[hint] LOG_DIR =', LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c9831d16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9831d16",
    "outputId": "6308f967-884a-4893-ea07-d87326586fa1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "playbooks root = /content/src_gcs/playbooks\n",
      "frames root    = /content/src_gcs/frames\n",
      "[render] 2023-06 - fastbreakOne - Expert Guide - Ecosystem Strategy  - English : pages= 62\n",
      "[render] PIK - Expert Guide - Platform IT Architecture - Playbook - v11 : pages= 50\n",
      "[render] PIK 5-0 - Introduction - English : pages= 49\n",
      "Prepared images: 186\n",
      "Manifest written to: /content/full_run_manifest.jsonl\n"
     ]
    }
   ],
   "source": [
    "#@title Full Run — enumerate playbooks and frames; render all pages\n",
    "\n",
    "require_start()\n",
    "import os, re, glob, json, subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "PLAYBOOKS_DIRS = ['/content/src_gcs/playbooks','/content/playbooks','/content/drive/MyDrive/playbooks']\n",
    "FRAMES_DIRS   = ['/content/src_gcs/frames','/content/frames','/content/drive/MyDrive/frames']\n",
    "OUT_PAGES_DIR = '/content/pages'\n",
    "MANIFEST      = '/content/full_run_manifest.jsonl'\n",
    "Path(OUT_PAGES_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _first_existing(dirs):\n",
    "  for d in dirs:\n",
    "    if Path(d).exists():\n",
    "      return d\n",
    "  return None\n",
    "\n",
    "pb_root = _first_existing(PLAYBOOKS_DIRS)\n",
    "fr_root = _first_existing(FRAMES_DIRS)\n",
    "print('playbooks root =', pb_root)\n",
    "print('frames root    =', fr_root)\n",
    "\n",
    "def _pdf_pages(pdf_path: str) -> int:\n",
    "  try:\n",
    "    out = subprocess.check_output(['pdfinfo', pdf_path], text=True)\n",
    "    m = re.search(r'^Pages:\\s+(\\d+)', out, re.M)\n",
    "    return int(m.group(1)) if m else 0\n",
    "  except Exception:\n",
    "    return 0\n",
    "\n",
    "images = []\n",
    "if pb_root:\n",
    "  for pdf in sorted(glob.glob(os.path.join(pb_root, '*.pdf'))):\n",
    "    name = Path(pdf).stem\n",
    "    out_dir = Path(OUT_PAGES_DIR)/name\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    n = _pdf_pages(pdf)\n",
    "    print('[render]', name, ': pages=', n)\n",
    "    for p in range(1, n+1):\n",
    "      png = out_dir/f'page-{p}.png'\n",
    "      if not png.exists():\n",
    "        subprocess.run(['pdftoppm','-f',str(p),'-l',str(p),'-png','-singlefile','-r','150', pdf, str(png.with_suffix(''))], check=True)\n",
    "      images.append(str(png))\n",
    "\n",
    "if fr_root:\n",
    "  for ext in ('*.png','*.jpg','*.jpeg'):\n",
    "    for fp in sorted(glob.glob(os.path.join(fr_root, '**', ext), recursive=True)):\n",
    "      images.append(fp)\n",
    "\n",
    "with open(MANIFEST,'w') as f:\n",
    "  for im in images:\n",
    "    f.write(json.dumps({'image': im})+'\\n')\n",
    "\n",
    "print('Prepared images:', len(images))\n",
    "print('Manifest written to:', MANIFEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e27f4a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e27f4a4",
    "outputId": "7020bae8-0d41-4bbb-86e6-0bbb1668cf72"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".. processed: 20 / 186\n",
      ".. processed: 40 / 186\n",
      ".. processed: 60 / 186\n",
      ".. processed: 80 / 186\n",
      ".. processed: 100 / 186\n",
      ".. processed: 120 / 186\n",
      ".. processed: 140 / 186\n",
      ".. processed: 160 / 186\n",
      ".. processed: 180 / 186\n",
      "[done] images: 186 errors: 0\n",
      "[local regions root]: /content/grounded_regions\n"
     ]
    }
   ],
   "source": [
    "#@title Batch Detect — iterate manifest and write regions (playbooks+frames)\n",
    "require_start()\n",
    "import os, json, base64, shutil, traceback\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Константы путей\n",
    "MANIFEST   = '/content/full_run_manifest.jsonl'\n",
    "DETECT_OUT = '/content/grounded_regions'\n",
    "\n",
    "# Корни, по которым определяем unit (чтоб группировать выход)\n",
    "PAGES_PB   = '/content/pages_src/playbooks'\n",
    "PAGES_FR   = '/content/pages_src/frames'\n",
    "FRAMES_RAW = '/content/src_gcs/frames'  # смонтированный источник PNG\n",
    "\n",
    "Path(DETECT_OUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Загрузка манифеста\n",
    "images = []\n",
    "mp = Path(MANIFEST)\n",
    "if not mp.exists():\n",
    "    raise SystemExit(f'[fatal] manifest not found: {MANIFEST}')\n",
    "with mp.open('r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        try:\n",
    "            images.append(json.loads(line)['image'])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def _unit_from_image(img: str) -> str:\n",
    "    p = Path(img)\n",
    "    s = str(p)\n",
    "    if s.startswith(PAGES_PB):\n",
    "        # /content/pages_src/playbooks/<doc>/page-*.png\n",
    "        parts = p.parts\n",
    "        idx = len(Path(PAGES_PB).parts)\n",
    "        doc = parts[idx] if len(parts) > idx else p.stem\n",
    "        return f'playbooks/{doc}'\n",
    "    if s.startswith(PAGES_FR):\n",
    "        # /content/pages_src/frames/<doc>/page-*.png\n",
    "        parts = p.parts\n",
    "        idx = len(Path(PAGES_FR).parts)\n",
    "        doc = parts[idx] if len(parts) > idx else p.stem\n",
    "        return f'frames-pdf/{doc}'\n",
    "    if s.startswith(FRAMES_RAW):\n",
    "        # /content/src_gcs/frames/**/<name>.png → использовать <parent>/<stem> если есть parent\n",
    "        parent = p.parent.name\n",
    "        if parent and parent not in ('frames',):\n",
    "            return f'frames/{parent}-{p.stem}'\n",
    "        return f'frames/{p.stem}'\n",
    "    return 'misc'\n",
    "\n",
    "def _ensure_dir(d: Path):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _png_b64(img_path: str) -> str:\n",
    "    try:\n",
    "        im = Image.open(img_path).convert('RGB')\n",
    "        import io\n",
    "        buf = io.BytesIO()\n",
    "        im.save(buf, format='PNG')\n",
    "        return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "    except Exception:\n",
    "        return ''\n",
    "\n",
    "def _placeholder_detect(img_path: str, unit_dir: Path):\n",
    "    rdir = unit_dir / 'regions'\n",
    "    _ensure_dir(rdir)\n",
    "    # region-1.json\n",
    "    b64 = _png_b64(img_path)\n",
    "    (rdir / 'region-1.json').write_text(\n",
    "        json.dumps({'bbox': {'x':0,'y':0,'w':-1,'h':-1},\n",
    "                    'text': '',\n",
    "                    'image_b64': b64}, ensure_ascii=False),\n",
    "        encoding='utf-8'\n",
    "    )\n",
    "    # preview region-1.png\n",
    "    try:\n",
    "        dst = rdir / 'region-1.png'\n",
    "        if not dst.exists():\n",
    "            shutil.copy2(img_path, dst)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Детектор если объявлен в ноутбуке\n",
    "det_fn = globals().get('detect_image_to_regions') or globals().get('run_detection_one')\n",
    "\n",
    "errors = 0\n",
    "for i, im in enumerate(images, start=1):\n",
    "    try:\n",
    "        unit = _unit_from_image(im)\n",
    "        unit_dir = Path(DETECT_OUT) / unit\n",
    "        if callable(det_fn):\n",
    "            try:\n",
    "                det_fn(im, str(unit_dir))  # ожидаем сигнатуру (image_path, out_dir)\n",
    "            except TypeError:\n",
    "                # на всякий случай поддержим вариант (image_path) и пишем в unit_dir/regions внутри\n",
    "                det_fn(im)\n",
    "        else:\n",
    "            _placeholder_detect(im, unit_dir)\n",
    "        if (i % 20) == 0:\n",
    "            print('.. processed:', i, '/', len(images))\n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        print('[error] on', im, ':', e)\n",
    "        traceback.print_exc()\n",
    "\n",
    "print('[done] images:', len(images), 'errors:', errors)\n",
    "print('[local regions root]:', DETECT_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "01aafa53",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01aafa53",
    "outputId": "919f71b9-2463-48ce-f3fd-b65f52dced8a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--- Run Summary ---\n",
      "Images in manifest        : 186\n",
      "Rendered pages PNG        : 190\n",
      "Region JSON files         : 113\n",
      "Region preview PNG files  : 113\n",
      "Units (sample)            : ['PIK - Expert Guide - Platform IT Architecture - Assessment - v01', 'PIK - Platform IT Architecture Canvas - Table View - v01', 'PIK - Platform IT Architecture Canvases - v01', 'PIK 5-0 - Ecosystem Forces Scan - ENG', 'PIK 5-0 - Longtail Discovery - ENG', 'PIK 5-0 - MVP - ENG', 'PIK 5-0 - Market Sizing - ENG', 'PIK 5-0 - NFX Reinforcement Engines - ENG', 'PIK 5-0 - Network Effects Stimulation - ENG', 'PIK 5-0 - Platform AARRR Funnel - ENG'] ...\n",
      "Local regions root        : /content/grounded_regions\n",
      "Artifacts (fuse) root     : /content/artifacts\n",
      "Hint remote prefix        : /content/artifacts/grounded_regions\n"
     ]
    }
   ],
   "source": [
    "#@title Run Summary — counts and outputs\n",
    "require_start()\n",
    "import os, glob, json\n",
    "from pathlib import Path\n",
    "\n",
    "MANIFEST   = '/content/full_run_manifest.jsonl'\n",
    "DETECT_OUT = '/content/grounded_regions'\n",
    "PAGES_ROOT = '/content/pages_src'  # включает /playbooks и /frames\n",
    "ARTIFACTS_MOUNT = Path('/content/artifacts')\n",
    "GCS_BUCKET = os.environ.get('GCS_BUCKET', 'pik-artifacts-dev')\n",
    "DEST_PREFIX= os.environ.get('DEST_PREFIX', 'colab_runs')\n",
    "\n",
    "# Манифест\n",
    "images_total = 0\n",
    "mp = Path(MANIFEST)\n",
    "if mp.exists():\n",
    "    with mp.open('r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): images_total += 1\n",
    "\n",
    "# Страницы\n",
    "rendered_pages = len(glob.glob(os.path.join(PAGES_ROOT, '**', 'page-*.png'), recursive=True))\n",
    "\n",
    "# Регионы\n",
    "region_jsons  = len(glob.glob(os.path.join(DETECT_OUT, '**', 'regions', 'region-*.json'), recursive=True))\n",
    "region_pngs   = len(glob.glob(os.path.join(DETECT_OUT, '**', 'regions', 'region-*.png'), recursive=True))\n",
    "\n",
    "# Unit’ы (папки непосредственно под grounded_regions)\n",
    "units = []\n",
    "gr = Path(DETECT_OUT)\n",
    "if gr.exists():\n",
    "    for d in gr.iterdir():\n",
    "        if d.is_dir():\n",
    "            units.append(d.name)\n",
    "units_sorted = sorted(units)\n",
    "\n",
    "print('--- Run Summary ---')\n",
    "print('Images in manifest        :', images_total)\n",
    "print('Rendered pages PNG        :', rendered_pages)\n",
    "print('Region JSON files         :', region_jsons)\n",
    "print('Region preview PNG files  :', region_pngs)\n",
    "print('Units (sample)            :', units_sorted[:10], ('...' if len(units_sorted)>10 else ''))\n",
    "print('Local regions root        :', DETECT_OUT)\n",
    "\n",
    "if ARTIFACTS_MOUNT.exists():\n",
    "    print('Artifacts (fuse) root     :', ARTIFACTS_MOUNT)\n",
    "    # Подсказка куда чаще всего выгружаем регионы через Upload Regions\n",
    "    run_id = os.environ.get('RUN_ID', '(set RUN_ID for subdir)')\n",
    "    print('Hint remote prefix        :', ARTIFACTS_MOUNT / f'grounded_regions')\n",
    "else:\n",
    "    print('Artifacts (fuse) root     : not mounted')\n",
    "    print('Hint remote URI           :', f'gs://{GCS_BUCKET}/grounded_regions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "bRgB4kFNi3QE"
   },
   "id": "bRgB4kFNi3QE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Batch Detect — iterate manifest and write regions (+optional GCS upload)\n",
    "\n",
    "require_start()\n",
    "import os, json, shutil, base64\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "MANIFEST = '/content/full_run_manifest.jsonl'\n",
    "DETECT_OUT = '/content/grounded_regions'\n",
    "PROMPTS = ['diagram','canvas','table','legend','arrow','node']\n",
    "Path(DETECT_OUT).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _png_b64(img_path: str) -> str:\n",
    "  try:\n",
    "    im = Image.open(img_path).convert('RGB')\n",
    "    import io\n",
    "    buf = io.BytesIO(); im.save(buf, format='PNG')\n",
    "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "  except Exception:\n",
    "    return ''\n",
    "\n",
    "def _fallback_detect(img_path: str, unit_dir: Path):\n",
    "  (unit_dir/'regions').mkdir(parents=True, exist_ok=True)\n",
    "  reg = unit_dir/'regions'/'region-1.json'\n",
    "  b64 = _png_b64(img_path)\n",
    "  reg.write_text(json.dumps({\n",
    "    'bbox': {'x':0,'y':0,'w':-1,'h':-1},\n",
    "    'text': '',\n",
    "    'image_b64': b64,\n",
    "  }), encoding='utf-8')\n",
    "  # copy preview\n",
    "  try:\n",
    "    dst = unit_dir/'regions'/'region-1.png'\n",
    "    if not dst.exists():\n",
    "      shutil.copy2(img_path, dst)\n",
    "  except Exception:\n",
    "    pass\n",
    "\n",
    "def _unit_from_path(p: Path) -> str:\n",
    "  # Use parent dir for pages/<doc>/page-<n>.png; else fall back to stem\n",
    "  if p.parent.name.startswith('page-'):\n",
    "    return p.parent.parent.name\n",
    "  if p.parent.name and p.parent.parent.name == 'pages':\n",
    "    return p.parent.name\n",
    "  return p.stem\n",
    "\n",
    "def detect_image(img_path: str):\n",
    "  # Try to call a notebook-level detect function if present; else fallback placeholder\n",
    "  nb = globals()\n",
    "  unit = _unit_from_path(Path(img_path))\n",
    "  unit_dir = Path(DETECT_OUT)/unit\n",
    "  fn = nb.get('detect_image_to_regions') or nb.get('run_detection_one')\n",
    "  if callable(fn):\n",
    "    try:\n",
    "      fn(img_path, str(unit_dir))\n",
    "      return\n",
    "    except Exception as e:\n",
    "      print('[warn] detect func failed, fallback:', e)\n",
    "  _fallback_detect(img_path, unit_dir)\n",
    "\n",
    "images = []\n",
    "with open(MANIFEST, 'r', encoding='utf-8') as f:\n",
    "  for line in f:\n",
    "    if line.strip():\n",
    "      obj = json.loads(line)\n",
    "      images.append(obj.get('image'))\n",
    "\n",
    "print('Detecting over', len(images), 'images')\n",
    "for i, im in enumerate(images, start=1):\n",
    "  try:\n",
    "    detect_image(im)\n",
    "    if i % 20 == 0:\n",
    "      print('..', i, 'done')\n",
    "  except Exception as e:\n",
    "    print('[error] failed on', im, e)\n",
    "\n",
    "print('Local regions root:', DETECT_OUT)\n",
    "\n",
    "# Optional: upload to GCS if bucket var present\n",
    "try:\n",
    "  import subprocess\n",
    "  bucket = GCS_BUCKET if 'GCS_BUCKET' in globals() else None\n",
    "  if bucket:\n",
    "    print('[upload] syncing to', f'gs://{bucket}/grounded_regions')\n",
    "    subprocess.run(['bash','-lc', f\"gsutil -m rsync -r '{DETECT_OUT}' 'gs://{bucket}/grounded_regions'\"], check=False)\n",
    "except Exception as e:\n",
    "  print('[warn] GCS upload failed:', e)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}