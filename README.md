# PIK‑AI Parsing MVP

This repository contains a proof of concept for a platform that ingests PIK methodology PDFs and exposes the raw document structure to a web interface. The goal of this milestone is to build a **functional skeleton** that can import documents, persist the extracted content in a SQLite database and provide a simple search and browsing experience. Subsequent iterations will layer on automatic mapping into PIK frames, vector search and RAG‑style Q&A.

## Project structure

```text
pik-ai/
├── app/               # Next.js pages and API routes
│   ├── api/
│   │   ├── ingest/
│   │   │   ├── route.ts              # POST /api/ingest – disabled (410 Gone), path-based ingest removed
│   │   │   └── upload/route.ts       # POST /api/ingest/upload – multipart upload (.pdf/.png)
│   │   ├── ingest/[id]/report/  # GET /api/ingest/:id/report – simple ingest report
│   │   │   └── route.ts
│   │   ├── search/    # GET /api/search for basic substring search over blocks
│   │   │   └── route.ts
│   │   ├── docs/      # API routes for listing and fetching documents and blocks
│   │   │   ├── route.ts      # GET /api/docs – list docs
│   │   │   └── [id]/route.ts          # GET /api/docs/:id – get doc and blocks
│   │   └── docs/[id]/pdf/route.ts     # GET /api/docs/:id/pdf – serve original PDF
│   ├── docs/          # UI pages for listing and viewing documents
│   │   ├── page.tsx   # /docs – list ingested documents
│   │   └── [id]/page.tsx # /docs/:id – show extracted blocks per page
│   ├── frames/        # UI page for PIK frames (lookup only)
│   │   └── page.tsx   # /frames – list frames & fields
│   └── page.tsx       # Home page linking to API endpoints and docs
├── lib/
│   └── pdf/
│       └── adobeExtract.ts  # Client to call Adobe PDF Services Extract API
├── prisma/
│   └── schema.prisma  # Prisma data model definitions
├── db/
│   └── (generated by Prisma migrations)
├── data/
│   ├── uploads/       # uploaded files (archived); gitignored, keeps .gitkeep
│   ├── raw/           # raw JSON output from Adobe Extract API (gitignored)
│   └── normalized/    # normalized blocks (gitignored)
├── .env.example       # example environment variables
├── package.json       # Node.js dependencies and scripts
├── tsconfig.json      # TypeScript configuration
└── next.config.js     # Next.js configuration
```text

## Quick start

1. **Install dependencies.**  This project uses Next.js with TypeScript and Prisma.  Run `npm install` in the `pik-ai` directory to install packages.  (Network access is required.)

2. **Configure environment.**  Copy `.env.example` to `.env.local` and fill in Adobe PDF Services product token credentials:

```env
ADOBE_CLIENT_ID=
ADOBE_CLIENT_SECRET=
# ADOBE_REGION=ew1 (optional)
```sh

4. **Run database migrations and seed lookup tables.**  Prisma uses a SQLite database stored in `./db`.  To create the database and apply the schema, run:

   ```sh
   npx prisma migrate dev --name init
   ```

   Then populate the PIK frame definitions:

   ```sh
   npx prisma db seed
   ```

1. **(Optional) Verify Adobe token.**

```sh
npm run adobe:token
# expected: "token ok, expires_in <number>"
```

3. **Start the dev server.**  Run the Next.js development server with:

   ```sh
   npm run dev
   ```

   Visit `http://localhost:3000` in your browser.  You can now:

   * Open **`/upload`** to upload a `.pdf` or `.png` (<= 30 MB). PNGs are converted via Create PDF first, then extracted.
   * Browse **`/docs`** for a list of uploaded documents; open an item to view extracted blocks grouped by page with a PDF overlay.
   * Use **`/api/search?q=…`** for a basic case‑insensitive substring search over the extracted text.  The endpoint returns up to 20 matching blocks with the document id, page and a snippet.
   * View **`/frames`** to see the static definitions of PIK frames and their fields.  Automatic mapping into these frames is not yet implemented.
   
Health endpoint: `/api/health` → `{ ok, docs, blocks, time }`.

## Diagnostics

* Endpoint: `/api/diagnostics/pdfservices`
   * Resolves DNS (IPv4/IPv6) for `ims-na1.adobelogin.com` and `pdfservices.adobe.io`.
   * Performs a minimal TLS check to IMS (POST `/ims/token/v3`, expected 400/401) and to PDF Services (GET `/assets`, expected 401/404/405).
   * If `ADOBE_CLIENT_ID`/`ADOBE_API_KEY` is set, attempts POST `/assets` with `x-api-key` (without Bearer). Secrets are not logged.
   * Node runtime prefers IPv4 by default via `dns.setDefaultResultOrder('ipv4first')` to avoid IPv6‑only failures.

### How to interpret results

* IMS `POST /ims/token/v3`:
   * 400/401 → соединение и TLS в порядке (ожидаемо, т.к. нет корректного тела или Bearer). Значит сеть доступна.
   * fetch failed / DNS errors → проблема соединения или резолва.

* PDF Services `GET /assets`:
   * 401/404/405 → соединение ок (ожидаемо без авторизации/метода). Сеть до `pdfservices.adobe.io` есть.
   * 403 → ключ/права на стороне Adobe (или IP ограничение). Проверьте проект в Adobe Console.
   * fetch failed / DNS errors → проблема соединения или резолва.

- PDF Services `POST /assets` с `x-api-key` (без Bearer):
  - 401 → ожидаемо (нет Bearer), `x-api-key` принят транспортно.
  - 403 → неверный `x-api-key` или проект/права не активны.
  - 201/202 → ок только при полном наборе заголовков+Bearer (в диагностике не отправляется). 

## Demo Steps

1. Generate Prisma client, migrate DB, and seed frames:

   ```sh
   npm run prisma:generate && npm run db:migrate && npm run db:seed
   ```

2. Verify Adobe token (Server-to-Server OAuth):

   ```sh
   npm run adobe:token
   # expected: token ok, expires_in <seconds>
   ```

3. Start the dev server and open `/upload`, then upload your file. After completion you’ll be redirected to `/docs/:id`.

## Adobe Extract implementation

`lib/pdf/adobeExtract.ts` implements the **async job flow** (asset upload → start job → polling → result download). Helper `extractWithRawJob` returns `{ raw, blocks }`. PNG uploads are converted via `Create PDF` before extraction.

## Prisma data model

The Prisma schema defines several entities:

* **`SourceDoc`** – metadata about each ingested document (title, file type, file system path, number of pages and creation timestamp).
* **`Block`** – a region of content extracted from the PDF, with page number, bounding box (`bbox`), role (e.g. heading, paragraph, list, table), plain text and optional table JSON.  Each block belongs to a `SourceDoc`.  A simple index on `text` enables substring searches.
* **`Methodology`, `Frame` and `Field`** – static lookup tables describing the PIK methodology.  These are populated via seed scripts.  The `Methodology` model is unique per combination of title and version, and each `Field` has a unique slug.

## Scripts

| Script | Purpose |
|--------|---------|
| `dev` | Start Next.js dev server (port 3000 or next free) |
| `dev:3001` | Dev server on port 3001 |
| `build` | Production build |
| `start` | Start production server |
| `prisma:generate` | Generate Prisma client |
| `db:migrate` | Run dev migration (creates / updates schema) |
| `db:seed` | Seed frames / lookup data |
| `adobe:token` | Obtain product token (sanity check) |

## Environment variables

| Variable | Required (prod) | Description |
|----------|-----------------|-------------|
| `ADOBE_CLIENT_ID` | Yes | Adobe PDF Services client id (`x-api-key`) |
| `ADOBE_CLIENT_SECRET` | Yes | Adobe PDF Services client secret |
| `ADOBE_REGION` | No | Optional region override (e.g., `ew1`, `ue1`) |

`*` Required for real (non-mock) ingestion.

## What’s missing

Even with the additions in this milestone, many features remain to be implemented:

* **Mapping into PIK frames** – currently the frames serve only as a static lookup; there is no automatic assignment of blocks to fields.
* **Hybrid search and vector embeddings** – the search endpoint performs a naive substring match rather than true full‑text (FTS5) or vector search.
* **RAG Q&A endpoints** – there is no retrieval‑augmented generation yet.
* **Additional ingest providers** – only Adobe Extract implemented (job flow). Future: Unstructured, LlamaParse.
* **Frame mapping automation** – blocks not yet mapped into Frame/Field entities.
* **Production deployment** – scripts for building and deploying to cloud platforms or containers are not yet provided.

The intention is to build incrementally – first establish a solid ingestion and browsing foundation, then iteratively add richer semantics (frame mapping, search, RAG) and automation.

---

> Note: Avoid adding `pages/index.tsx` alongside `app/page.tsx` — this creates a routing conflict in Next.js 14 (build will fail with a "Conflicting app and page file" error).
